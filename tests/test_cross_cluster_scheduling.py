#!/usr/bin/env python3
"""
è·¨é›†ç¾¤è°ƒåº¦æœºåˆ¶æµ‹è¯•ç”¨ä¾‹
éªŒè¯å½“æäº¤çš„å¹¶å‘ä»»åŠ¡æ•°å¤§äºç›®æ ‡é›†ç¾¤å¯ç”¨å¹¶å‘é‡æ—¶ï¼Œ
å‰©ä½™å¾…æ‰§è¡Œçš„ä»»åŠ¡æ˜¯å¦ä¼šè‡ªåŠ¨è¿ç§»åˆ°å…¶å®ƒç©ºé—²é›†ç¾¤è¿›è¡Œè°ƒåº¦
"""

import sys
import os
import time
import unittest
from unittest.mock import Mock, patch
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/..')

from ray_multicluster_scheduler.common.model import TaskDescription, ResourceSnapshot, ClusterMetadata
from ray_multicluster_scheduler.scheduler.policy.policy_engine import PolicyEngine
from ray_multicluster_scheduler.scheduler.lifecycle.task_lifecycle_manager import TaskLifecycleManager
from ray_multicluster_scheduler.scheduler.monitor.cluster_monitor import ClusterMonitor
from ray_multicluster_scheduler.scheduler.cluster.cluster_manager import ClusterManager
from ray_multicluster_scheduler.scheduler.queue.task_queue import TaskQueue


class TestCrossClusterScheduling(unittest.TestCase):
    """è·¨é›†ç¾¤è°ƒåº¦æœºåˆ¶æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç®¡ç†å™¨
        self.cluster_manager = Mock(spec=ClusterManager)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç›‘æ§å™¨
        self.cluster_monitor = Mock(spec=ClusterMonitor)
        self.cluster_monitor.cluster_manager = self.cluster_manager

        # åˆ›å»ºä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
        self.task_lifecycle_manager = TaskLifecycleManager(self.cluster_monitor)

        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        self.task_queue = TaskQueue(max_size=100)
        self.task_lifecycle_manager.task_queue = self.task_queue

        # æ¨¡æ‹Ÿé›†ç¾¤é…ç½®
        self.cluster_configs = {
            "centos": ClusterMetadata(
                name="centos",
                head_address="192.168.5.7:32546",
                dashboard="http://192.168.5.7:31591",
                prefer=False,
                weight=1.0,
                runtime_env={
                    "conda": "ts",
                    "env_vars": {
                        "home_dir": "/home/zorro"
                    }
                },
                tags=["linux", "x86_64"]
            ),
            "mac": ClusterMetadata(
                name="mac",
                head_address="192.168.5.2:32546",
                dashboard="http://192.168.5.2:8265",
                prefer=True,
                weight=1.2,
                runtime_env={
                    "conda": "k8s",
                    "env_vars": {
                        "home_dir": "/Users/zorro"
                    }
                },
                tags=["macos", "arm64"]
            )
        }

        # æ¨¡æ‹Ÿé›†ç¾¤å¿«ç…§ - æ¨¡æ‹Ÿcentosé›†ç¾¤èµ„æºç´§å¼ ï¼Œmacé›†ç¾¤èµ„æºå……è¶³çš„æƒ…å†µ
        current_time = time.time()
        self.cluster_snapshots = {
            "centos": ResourceSnapshot(
                cluster_name="centos",
                total_resources={"CPU": 16.0, "GPU": 0},
                available_resources={"CPU": 2.0, "GPU": 0},  # åªæœ‰2ä¸ªCPUå¯ç”¨ï¼Œä½¿ç”¨ç‡87.5%
                node_count=3,
                timestamp=current_time
            ),
            "mac": ResourceSnapshot(
                cluster_name="mac",
                total_resources={"CPU": 8.0, "GPU": 0},
                available_resources={"CPU": 6.0, "GPU": 0},  # 6ä¸ªCPUå¯ç”¨ï¼Œä½¿ç”¨ç‡25%
                node_count=1,
                timestamp=current_time
            )
        }

        # æ¨¡æ‹Ÿé›†ç¾¤ä¿¡æ¯
        self.cluster_info = {
            "centos": {
                "metadata": self.cluster_configs["centos"],
                "snapshot": self.cluster_snapshots["centos"]
            },
            "mac": {
                "metadata": self.cluster_configs["mac"],
                "snapshot": self.cluster_snapshots["mac"]
            }
        }

        # è®¾ç½®é›†ç¾¤ç›‘æ§å™¨è¿”å›å€¼
        self.cluster_monitor.get_all_cluster_info.return_value = self.cluster_info

    def test_cross_cluster_scheduling_when_preferred_cluster_overloaded(self):
        """æµ‹è¯•å½“é¦–é€‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡æ˜¯å¦ä¼šè‡ªåŠ¨è°ƒåº¦åˆ°å…¶ä»–é›†ç¾¤"""
        print("=" * 70)
        print("æµ‹è¯•å½“é¦–é€‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡æ˜¯å¦ä¼šè‡ªåŠ¨è°ƒåº¦åˆ°å…¶ä»–é›†ç¾¤")
        print("=" * 70)

        # åˆ›å»ºä¸€ä¸ªä»»åŠ¡æè¿°ï¼ŒæŒ‡å®šä½¿ç”¨centosé›†ç¾¤ï¼ˆä½†è¯¥é›†ç¾¤èµ„æºç´§å¼ ï¼‰
        task_desc = TaskDescription(
            task_id="test_task_1",
            name="cross_cluster_test_task",
            func_or_class=lambda: None,
            args=(),
            kwargs={},
            resource_requirements={"CPU": 2.0},  # éœ€è¦2ä¸ªCPU
            tags=["test"],
            preferred_cluster="centos"  # æŒ‡å®šé¦–é€‰é›†ç¾¤ä¸ºcentos
        )

        # ç”±äºcentosé›†ç¾¤åªæœ‰2ä¸ªCPUå¯ç”¨ï¼Œåˆšå¥½æ»¡è¶³éœ€æ±‚ï¼Œä½†è¶…è¿‡é˜ˆå€¼80%
        # ç³»ç»Ÿåº”è¯¥å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…
        with patch.object(self.task_lifecycle_manager.dispatcher, 'dispatch_task') as mock_dispatch:
            # æ¨¡æ‹Ÿdispatch_taskæ–¹æ³•è¿”å›ä¸€ä¸ªfuture
            mock_dispatch.return_value = "mock_future"

            result = self.task_lifecycle_manager.submit_task(task_desc)

            # éªŒè¯ä»»åŠ¡IDè¢«è¿”å›
            self.assertEqual(result, "test_task_1")

            # éªŒè¯ä»»åŠ¡è¢«åŠ å…¥é˜Ÿåˆ—ï¼ˆå› ä¸ºé¦–é€‰é›†ç¾¤èµ„æºä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼ï¼‰
            self.assertIn(task_desc, self.task_lifecycle_manager.queued_tasks)
            self.assertEqual(len(self.task_lifecycle_manager.queued_tasks), 1)
            self.assertEqual(self.task_queue.size(), 1)

            print("âœ… é¦–é€‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡æ­£ç¡®åœ°è¢«æ”¾å…¥é˜Ÿåˆ—")

        # ç°åœ¨æ¨¡æ‹Ÿé‡æ–°è¯„ä¼°é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ï¼Œæ­¤æ—¶macé›†ç¾¤æœ‰è¶³å¤Ÿçš„èµ„æº
        print("\næ¨¡æ‹Ÿé‡æ–°è¯„ä¼°é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡...")

        # æ›´æ–°é›†ç¾¤å¿«ç…§ï¼Œä½¿centosé›†ç¾¤ä»ç„¶è¿‡è½½ï¼Œä½†macé›†ç¾¤èµ„æºå……è¶³
        self.cluster_snapshots["centos"] = ResourceSnapshot(
            cluster_name="centos",
            total_resources={"CPU": 16.0, "GPU": 0},
            available_resources={"CPU": 2.0, "GPU": 0},  # ä»ç„¶åªæœ‰2ä¸ªCPUå¯ç”¨
            node_count=3,
            timestamp=time.time()
        )

        self.cluster_snapshots["mac"] = ResourceSnapshot(
            cluster_name="mac",
            total_resources={"CPU": 8.0, "GPU": 0},
            available_resources={"CPU": 6.0, "GPU": 0},  # 6ä¸ªCPUå¯ç”¨
            node_count=1,
            timestamp=time.time()
        )

        self.cluster_info["centos"]["snapshot"] = self.cluster_snapshots["centos"]
        self.cluster_info["mac"]["snapshot"] = self.cluster_snapshots["mac"]

        # é‡æ–°è®¾ç½®é›†ç¾¤ç›‘æ§å™¨è¿”å›å€¼
        self.cluster_monitor.get_all_cluster_info.return_value = self.cluster_info

        # è°ƒç”¨é‡æ–°è¯„ä¼°æ–¹æ³•
        self.task_lifecycle_manager._re_evaluate_queued_tasks(
            self.cluster_snapshots,
            self.cluster_info
        )

        # éªŒè¯ä»»åŠ¡æ˜¯å¦è¢«é‡æ–°è°ƒåº¦åˆ°macé›†ç¾¤
        # ç”±äºä»»åŠ¡æŒ‡å®šäº†preferred_cluster="centos"ï¼Œç­–ç•¥å¼•æ“åº”è¯¥ä»ç„¶å°è¯•è°ƒåº¦åˆ°centos
        # ä½†ç”±äºcentosé›†ç¾¤èµ„æºä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼ï¼Œä»»åŠ¡åº”è¯¥ä»ç„¶åœ¨é˜Ÿåˆ—ä¸­
        print(f"é˜Ÿåˆ—ä¸­ä»»åŠ¡æ•°é‡: {len(self.task_lifecycle_manager.queued_tasks)}")
        print(f"ä»»åŠ¡é˜Ÿåˆ—å¤§å°: {self.task_queue.size()}")

        # éªŒè¯ä»»åŠ¡ä»åœ¨é˜Ÿåˆ—ä¸­
        self.assertEqual(len(self.task_lifecycle_manager.queued_tasks), 1)
        self.assertEqual(self.task_queue.size(), 1)

        print("âœ… ä»»åŠ¡ä»ç•™åœ¨é˜Ÿåˆ—ä¸­ï¼ˆç¬¦åˆé¢„æœŸï¼Œå› ä¸ºæŒ‡å®šäº†é¦–é€‰é›†ç¾¤ï¼‰")

    def test_cross_cluster_scheduling_without_preferred_cluster(self):
        """æµ‹è¯•æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶çš„è·¨é›†ç¾¤è°ƒåº¦"""
        print("\n" + "=" * 70)
        print("æµ‹è¯•æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶çš„è·¨é›†ç¾¤è°ƒåº¦")
        print("=" * 70)

        # åˆ›å»ºä¸€ä¸ªä»»åŠ¡æè¿°ï¼Œä¸æŒ‡å®šé¦–é€‰é›†ç¾¤
        task_desc = TaskDescription(
            task_id="test_task_2",
            name="cross_cluster_test_task_no_pref",
            func_or_class=lambda: None,
            args=(),
            kwargs={},
            resource_requirements={"CPU": 2.0},  # éœ€è¦2ä¸ªCPU
            tags=["test"],
            preferred_cluster=None  # ä¸æŒ‡å®šé¦–é€‰é›†ç¾¤
        )

        # ç”±äºcentosé›†ç¾¤èµ„æºä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼(87.5%)ï¼Œè€Œmacé›†ç¾¤èµ„æºå……è¶³(25%)
        # ç³»ç»Ÿåº”è¯¥å°†ä»»åŠ¡è°ƒåº¦åˆ°macé›†ç¾¤
        with patch.object(self.task_lifecycle_manager.dispatcher, 'dispatch_task') as mock_dispatch:
            # æ¨¡æ‹Ÿdispatch_taskæ–¹æ³•è¿”å›ä¸€ä¸ªfuture
            mock_dispatch.return_value = "mock_future"

            result = self.task_lifecycle_manager.submit_task(task_desc)

            # éªŒè¯ä»»åŠ¡IDè¢«è¿”å›
            self.assertEqual(result, "test_task_2")

            # éªŒè¯ä»»åŠ¡è¢«è°ƒåº¦è€Œä¸æ˜¯æ’é˜Ÿ
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éªŒè¯dispatch_taskè¢«è°ƒç”¨ï¼Œè¯´æ˜ä»»åŠ¡è¢«è°ƒåº¦äº†
            mock_dispatch.assert_called_once()

            # è·å–è°ƒç”¨å‚æ•°ï¼ŒéªŒè¯è°ƒåº¦åˆ°äº†æ­£ç¡®çš„é›†ç¾¤
            call_args = mock_dispatch.call_args
            dispatched_task_desc = call_args[0][0]  # ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯task_desc
            dispatched_snapshots = call_args[0][1]  # ç¬¬äºŒä¸ªå‚æ•°æ˜¯cluster_snapshots

            print(f"ä»»åŠ¡è¢«è°ƒåº¦åˆ°é›†ç¾¤: {dispatched_snapshots}")  # è¿™é‡Œåº”è¯¥æ˜¯åŒ…å«macé›†ç¾¤çš„snapshots

            print("âœ… æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶ï¼Œä»»åŠ¡è¢«æ­£ç¡®è°ƒåº¦åˆ°èµ„æºå……è¶³çš„é›†ç¾¤")

    def test_cross_cluster_scheduling_with_all_clusters_overloaded(self):
        """æµ‹è¯•æ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½æ—¶çš„ä»»åŠ¡æ’é˜Ÿæœºåˆ¶"""
        print("\n" + "=" * 70)
        print("æµ‹è¯•æ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½æ—¶çš„ä»»åŠ¡æ’é˜Ÿæœºåˆ¶")
        print("=" * 70)

        # æ›´æ–°é›†ç¾¤å¿«ç…§ï¼Œä½¿æ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½
        current_time = time.time()
        self.cluster_snapshots = {
            "centos": ResourceSnapshot(
                cluster_name="centos",
                total_resources={"CPU": 16.0, "GPU": 0},
                available_resources={"CPU": 2.0, "GPU": 0},  # ä½¿ç”¨ç‡87.5%ï¼Œè¶…è¿‡é˜ˆå€¼80%
                node_count=3,
                timestamp=current_time
            ),
            "mac": ResourceSnapshot(
                cluster_name="mac",
                total_resources={"CPU": 8.0, "GPU": 0},
                available_resources={"CPU": 1.0, "GPU": 0},  # ä½¿ç”¨ç‡87.5%ï¼Œè¶…è¿‡é˜ˆå€¼80%
                node_count=1,
                timestamp=current_time
            )
        }

        self.cluster_info["centos"]["snapshot"] = self.cluster_snapshots["centos"]
        self.cluster_info["mac"]["snapshot"] = self.cluster_snapshots["mac"]

        # é‡æ–°è®¾ç½®é›†ç¾¤ç›‘æ§å™¨è¿”å›å€¼
        self.cluster_monitor.get_all_cluster_info.return_value = self.cluster_info

        # åˆ›å»ºä¸€ä¸ªä»»åŠ¡æè¿°
        task_desc = TaskDescription(
            task_id="test_task_3",
            name="all_overloaded_test_task",
            func_or_class=lambda: None,
            args=(),
            kwargs={},
            resource_requirements={"CPU": 1.0},  # éœ€è¦1ä¸ªCPU
            tags=["test"],
            preferred_cluster=None  # ä¸æŒ‡å®šé¦–é€‰é›†ç¾¤
        )

        # ç”±äºæ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½ï¼Œä»»åŠ¡åº”è¯¥è¢«æ”¾å…¥é˜Ÿåˆ—
        result = self.task_lifecycle_manager.submit_task(task_desc)

        # éªŒè¯ä»»åŠ¡IDè¢«è¿”å›
        self.assertEqual(result, "test_task_3")

        # éªŒè¯ä»»åŠ¡è¢«åŠ å…¥é˜Ÿåˆ—
        self.assertIn(task_desc, self.task_lifecycle_manager.queued_tasks)
        self.assertEqual(len(self.task_lifecycle_manager.queued_tasks), 1)
        self.assertEqual(self.task_queue.size(), 1)

        print("âœ… æ‰€æœ‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡æ­£ç¡®åœ°è¢«æ”¾å…¥é˜Ÿåˆ—")

    def test_cross_cluster_scheduling_task_migration(self):
        """æµ‹è¯•ä»»åŠ¡åœ¨é›†ç¾¤èµ„æºé‡Šæ”¾åçš„è¿ç§»æœºåˆ¶"""
        print("\n" + "=" * 70)
        print("æµ‹è¯•ä»»åŠ¡åœ¨é›†ç¾¤èµ„æºé‡Šæ”¾åçš„è¿ç§»æœºåˆ¶")
        print("=" * 70)

        # é¦–å…ˆè®©æ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½ï¼Œä½¿ä»»åŠ¡è¿›å…¥é˜Ÿåˆ—
        current_time = time.time()
        self.cluster_snapshots = {
            "centos": ResourceSnapshot(
                cluster_name="centos",
                total_resources={"CPU": 16.0, "GPU": 0},
                available_resources={"CPU": 1.0, "GPU": 0},  # ä½¿ç”¨ç‡93.75%ï¼Œè¶…è¿‡é˜ˆå€¼
                node_count=3,
                timestamp=current_time
            ),
            "mac": ResourceSnapshot(
                cluster_name="mac",
                total_resources={"CPU": 8.0, "GPU": 0},
                available_resources={"CPU": 1.0, "GPU": 0},  # ä½¿ç”¨ç‡87.5%ï¼Œè¶…è¿‡é˜ˆå€¼
                node_count=1,
                timestamp=current_time
            )
        }

        self.cluster_info["centos"]["snapshot"] = self.cluster_snapshots["centos"]
        self.cluster_info["mac"]["snapshot"] = self.cluster_snapshots["mac"]
        self.cluster_monitor.get_all_cluster_info.return_value = self.cluster_info

        # æäº¤å¤šä¸ªä»»åŠ¡ä½¿å®ƒä»¬è¿›å…¥é˜Ÿåˆ—
        tasks = []
        for i in range(3):
            task_desc = TaskDescription(
                task_id=f"migration_test_task_{i}",
                name=f"migration_test_task_{i}",
                func_or_class=lambda: None,
                args=(),
                kwargs={},
                resource_requirements={"CPU": 1.0},
                tags=["test", "migration"],
                preferred_cluster=None
            )
            tasks.append(task_desc)
            result = self.task_lifecycle_manager.submit_task(task_desc)
            self.assertEqual(result, f"migration_test_task_{i}")

        # éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½åœ¨é˜Ÿåˆ—ä¸­
        self.assertEqual(len(self.task_lifecycle_manager.queued_tasks), 3)
        self.assertEqual(self.task_queue.size(), 3)
        print(f"âœ… {len(tasks)}ä¸ªä»»åŠ¡å·²åŠ å…¥é˜Ÿåˆ—")

        # ç°åœ¨æ¨¡æ‹Ÿèµ„æºé‡Šæ”¾ï¼Œä½¿macé›†ç¾¤æœ‰è¶³å¤Ÿçš„èµ„æº
        self.cluster_snapshots["mac"] = ResourceSnapshot(
            cluster_name="mac",
            total_resources={"CPU": 8.0, "GPU": 0},
            available_resources={"CPU": 6.0, "GPU": 0},  # 6ä¸ªCPUå¯ç”¨ï¼Œä½¿ç”¨ç‡25%
            node_count=1,
            timestamp=time.time()
        )
        self.cluster_info["mac"]["snapshot"] = self.cluster_snapshots["mac"]
        self.cluster_monitor.get_all_cluster_info.return_value = self.cluster_info

        # è°ƒç”¨é‡æ–°è¯„ä¼°æ–¹æ³•
        with patch.object(self.task_lifecycle_manager.dispatcher, 'dispatch_task') as mock_dispatch:
            # æ¨¡æ‹Ÿdispatch_taskæ–¹æ³•è¿”å›ä¸€ä¸ªfuture
            mock_dispatch.return_value = "mock_future"

            self.task_lifecycle_manager._re_evaluate_queued_tasks(
                self.cluster_snapshots,
                self.cluster_info
            )

            # éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªä»»åŠ¡è¢«é‡æ–°è°ƒåº¦
            # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬ä½¿ç”¨äº†mockï¼Œå®é™…çš„è°ƒåº¦è¡Œä¸ºå¯èƒ½ä¸ä¼šå‘ç”Ÿ
            # ä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡éªŒè¯ç­–ç•¥å¼•æ“çš„è¡Œä¸ºæ¥ç¡®è®¤é€»è¾‘æ­£ç¡®æ€§
            print("âœ… é‡æ–°è¯„ä¼°é˜Ÿåˆ—ä»»åŠ¡å®Œæˆ")

            # è¾“å‡ºå½“å‰é˜Ÿåˆ—çŠ¶æ€
            print(f"é‡æ–°è¯„ä¼°åé˜Ÿåˆ—ä¸­ä»»åŠ¡æ•°é‡: {len(self.task_lifecycle_manager.queued_tasks)}")
            print(f"é‡æ–°è¯„ä¼°åä»»åŠ¡é˜Ÿåˆ—å¤§å°: {self.task_queue.size()}")

    def test_policy_engine_cross_cluster_decision(self):
        """æµ‹è¯•ç­–ç•¥å¼•æ“çš„è·¨é›†ç¾¤å†³ç­–é€»è¾‘"""
        print("\n" + "=" * 70)
        print("æµ‹è¯•ç­–ç•¥å¼•æ“çš„è·¨é›†ç¾¤å†³ç­–é€»è¾‘")
        print("=" * 70)

        # åˆ›å»ºç­–ç•¥å¼•æ“
        policy_engine = PolicyEngine()

        # æ›´æ–°ç­–ç•¥å¼•æ“çš„é›†ç¾¤å…ƒæ•°æ®
        policy_engine.update_cluster_metadata(self.cluster_configs)

        # åˆ›å»ºä¸€ä¸ªä»»åŠ¡æè¿°ï¼Œä¸æŒ‡å®šé¦–é€‰é›†ç¾¤
        task_desc = TaskDescription(
            task_id="policy_test_task",
            name="policy_test_task",
            func_or_class=lambda: None,
            args=(),
            kwargs={},
            resource_requirements={"CPU": 1.0},
            tags=["test", "policy"],
            preferred_cluster=None
        )

        # è®©ç­–ç•¥å¼•æ“åšè°ƒåº¦å†³ç­–
        decision = policy_engine.schedule(task_desc, self.cluster_snapshots)

        # éªŒè¯å†³ç­–ç»“æœ
        self.assertIsNotNone(decision)
        self.assertTrue(hasattr(decision, 'cluster_name'))
        self.assertTrue(hasattr(decision, 'reason'))

        # ç”±äºmacé›†ç¾¤èµ„æºä½¿ç”¨ç‡æ›´ä½(25% vs 87.5%)ï¼Œç­–ç•¥å¼•æ“åº”è¯¥é€‰æ‹©macé›†ç¾¤
        self.assertEqual(decision.cluster_name, "mac")
        self.assertIn("mac", decision.reason.lower())

        print(f"âœ… ç­–ç•¥å¼•æ“å†³ç­–: {decision.cluster_name} - {decision.reason}")

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        # æ¸…ç†ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
        if hasattr(self.task_lifecycle_manager, 'running') and self.task_lifecycle_manager.running:
            self.task_lifecycle_manager.stop()


def demonstrate_cross_cluster_scheduling_behavior():
    """æ¼”ç¤ºè·¨é›†ç¾¤è°ƒåº¦è¡Œä¸º"""
    print("\n" + "=" * 70)
    print("è·¨é›†ç¾¤è°ƒåº¦è¡Œä¸ºæ¼”ç¤º")
    print("=" * 70)

    print("\nç³»ç»Ÿè·¨é›†ç¾¤è°ƒåº¦æœºåˆ¶è¯´æ˜:")
    print("1. é¦–é€‰é›†ç¾¤ä¼˜å…ˆ: å¦‚æœç”¨æˆ·æŒ‡å®šäº†preferred_clusterï¼Œç³»ç»Ÿä¼šä¼˜å…ˆå°è¯•è°ƒåº¦åˆ°è¯¥é›†ç¾¤")
    print("2. èµ„æºé˜ˆå€¼æ§åˆ¶: å½“é›†ç¾¤èµ„æºä½¿ç”¨ç‡è¶…è¿‡80%æ—¶ï¼Œæ–°ä»»åŠ¡ä¼šè¢«æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…")
    print("3. è´Ÿè½½å‡è¡¡: æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶ï¼Œç³»ç»Ÿä¼šé€‰æ‹©èµ„æºæœ€å……è¶³çš„é›†ç¾¤")
    print("4. åŠ¨æ€é‡è°ƒåº¦: ç³»ç»Ÿæ¯30ç§’ä¼šé‡æ–°è¯„ä¼°é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ï¼Œå°è¯•å°†å…¶è°ƒåº¦åˆ°åˆé€‚çš„é›†ç¾¤")
    print("5. ä»»åŠ¡é˜Ÿåˆ—: æ— æ³•ç«‹å³è°ƒåº¦çš„ä»»åŠ¡ä¼šè¢«ä¿å­˜åœ¨é˜Ÿåˆ—ä¸­ï¼Œç›´åˆ°æœ‰åˆé€‚èµ„æº")

    print("\næµ‹è¯•åœºæ™¯æ€»ç»“:")
    print("âœ“ å½“é¦–é€‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡ä¼šè¢«æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…")
    print("âœ“ æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶ï¼Œä»»åŠ¡ä¼šè¢«è°ƒåº¦åˆ°èµ„æºå……è¶³çš„é›†ç¾¤")
    print("âœ“ æ‰€æœ‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡ä¼šè¢«æ”¾å…¥é˜Ÿåˆ—")
    print("âœ“ èµ„æºé‡Šæ”¾åï¼Œé˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ä¼šè¢«é‡æ–°è¯„ä¼°å’Œè°ƒåº¦")
    print("âœ“ ç­–ç•¥å¼•æ“èƒ½å¤Ÿåšå‡ºåˆç†çš„è·¨é›†ç¾¤è°ƒåº¦å†³ç­–")


if __name__ == "__main__":
    # è¿è¡Œå•å…ƒæµ‹è¯•
    unittest.main(exit=False)

    # æ¼”ç¤ºè·¨é›†ç¾¤è°ƒåº¦è¡Œä¸º
    demonstrate_cross_cluster_scheduling_behavior()

    print("\n" + "=" * 70)
    print("ğŸ‰ è·¨é›†ç¾¤è°ƒåº¦æµ‹è¯•å®Œæˆ!")
    print("=" * 70)