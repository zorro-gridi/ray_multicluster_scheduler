#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆèµ„æºé˜ˆå€¼é˜Ÿåˆ—æµ‹è¯•ç”¨ä¾‹
æµ‹è¯•å½“æ‰€æœ‰é›†ç¾¤èµ„æºä½¿ç”¨çŽ‡è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œæ–°æäº¤çš„ä»»åŠ¡æ˜¯å¦æ­£ç¡®æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…
"""

import sys
import os
import time as time_module
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/..')

from unittest.mock import Mock

from ray_multicluster_scheduler.common.model import TaskDescription, ResourceSnapshot
from ray_multicluster_scheduler.scheduler.policy.policy_engine import PolicyEngine
from ray_multicluster_scheduler.scheduler.lifecycle.task_lifecycle_manager import TaskLifecycleManager
from ray_multicluster_scheduler.scheduler.monitor.cluster_monitor import ClusterMonitor
from ray_multicluster_scheduler.scheduler.cluster.cluster_manager import ClusterManager


def test_resource_threshold_queue_functionality():
    """æµ‹è¯•èµ„æºé˜ˆå€¼é˜Ÿåˆ—åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•èµ„æºé˜ˆå€¼é˜Ÿåˆ—åŠŸèƒ½")
    print("=" * 60)

    # åˆ›å»ºæ¨¡æ‹Ÿå¯¹è±¡
    cluster_manager = Mock(spec=ClusterManager)
    cluster_monitor = Mock(spec=ClusterMonitor)
    cluster_monitor.cluster_manager = cluster_manager

    # åˆ›å»ºä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
    task_lifecycle_manager = TaskLifecycleManager(cluster_monitor)

    # æµ‹è¯•åœºæ™¯1: æ‰€æœ‰é›†ç¾¤èµ„æºä½¿ç”¨çŽ‡éƒ½è¶…è¿‡é˜ˆå€¼
    print("\nåœºæ™¯1: æ‰€æœ‰é›†ç¾¤èµ„æºä½¿ç”¨çŽ‡éƒ½è¶…è¿‡é˜ˆå€¼(80%)")
    print("-" * 40)

    current_time = time_module.time()
    cluster_snapshots_over_threshold = {
        "cluster1": ResourceSnapshot(
            cluster_name="cluster1",
            total_resources={"CPU": 4.0, "GPU": 0},
            available_resources={"CPU": 0.5, "GPU": 0},  # ä½¿ç”¨çŽ‡87.5%
            node_count=2,
            timestamp=current_time
        ),
        "cluster2": ResourceSnapshot(
            cluster_name="cluster2",
            total_resources={"CPU": 8.0, "GPU": 2.0},
            available_resources={"CPU": 1.0, "GPU": 0},  # ä½¿ç”¨çŽ‡87.5%
            node_count=3,
            timestamp=current_time
        )
    }

    cluster_info_over_threshold = {
        "cluster1": {
            "snapshot": cluster_snapshots_over_threshold["cluster1"],
            "metadata": Mock()
        },
        "cluster2": {
            "snapshot": cluster_snapshots_over_threshold["cluster2"],
            "metadata": Mock()
        }
    }

    # è®¾ç½®é›†ç¾¤ç›‘æŽ§å™¨è¿”å›žå€¼
    cluster_monitor.get_all_cluster_info.return_value = cluster_info_over_threshold

    # åˆ›å»ºä»»åŠ¡æè¿°
    task_desc1 = TaskDescription(
        task_id="task_over_threshold_1",
        name="over_threshold_task",
        func_or_class=lambda: None,
        args=(),
        kwargs={},
        resource_requirements={"CPU": 1.0},
        tags=["test"],
        preferred_cluster=None
    )

    # è°ƒç”¨submit_taskæ–¹æ³•
    result1 = task_lifecycle_manager.submit_task(task_desc1)

    # éªŒè¯ç»“æžœ
    print(f"ä»»åŠ¡ID: {result1}")
    print(f"é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡æ•°é‡: {len(task_lifecycle_manager.queued_tasks)}")
    print(f"ä»»åŠ¡é˜Ÿåˆ—å¤§å°: {task_lifecycle_manager.task_queue.size()}")

    # éªŒè¯ä»»åŠ¡è¢«åŠ å…¥é˜Ÿåˆ—
    assert result1 == "task_over_threshold_1"
    assert len(task_lifecycle_manager.queued_tasks) == 1
    assert task_lifecycle_manager.task_queue.size() == 1
    print("âœ… ä»»åŠ¡æ­£ç¡®åœ°è¢«æ”¾å…¥é˜Ÿåˆ—")

    # æµ‹è¯•åœºæ™¯2: éƒ¨åˆ†é›†ç¾¤èµ„æºä½¿ç”¨çŽ‡ä½ŽäºŽé˜ˆå€¼
    print("\nåœºæ™¯2: éƒ¨åˆ†é›†ç¾¤èµ„æºä½¿ç”¨çŽ‡ä½ŽäºŽé˜ˆå€¼(80%)")
    print("-" * 40)

    cluster_snapshots_under_threshold = {
        "cluster1": ResourceSnapshot(
            cluster_name="cluster1",
            total_resources={"CPU": 4.0, "GPU": 0},
            available_resources={"CPU": 0.5, "GPU": 0},  # ä½¿ç”¨çŽ‡87.5%ï¼Œè¶…è¿‡é˜ˆå€¼
            node_count=2,
            timestamp=current_time
        ),
        "cluster2": ResourceSnapshot(
            cluster_name="cluster2",
            total_resources={"CPU": 8.0, "GPU": 2.0},
            available_resources={"CPU": 6.0, "GPU": 0},  # ä½¿ç”¨çŽ‡25%ï¼Œä½ŽäºŽé˜ˆå€¼
            node_count=3,
            timestamp=current_time
        )
    }

    cluster_info_under_threshold = {
        "cluster1": {
            "snapshot": cluster_snapshots_under_threshold["cluster1"],
            "metadata": Mock()
        },
        "cluster2": {
            "snapshot": cluster_snapshots_under_threshold["cluster2"],
            "metadata": Mock()
        }
    }

    # è®¾ç½®é›†ç¾¤ç›‘æŽ§å™¨è¿”å›žå€¼
    cluster_monitor.get_all_cluster_info.return_value = cluster_info_under_threshold

    # åˆ›å»ºä»»åŠ¡æè¿°
    task_desc2 = TaskDescription(
        task_id="task_under_threshold_1",
        name="under_threshold_task",
        func_or_class=lambda: None,
        args=(),
        kwargs={},
        resource_requirements={"CPU": 1.0},
        tags=["test"],
        preferred_cluster=None
    )

    # è°ƒç”¨submit_taskæ–¹æ³•
    result2 = task_lifecycle_manager.submit_task(task_desc2)

    # éªŒè¯ç»“æžœ
    print(f"ä»»åŠ¡ID: {result2}")
    print(f"é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡æ•°é‡: {len(task_lifecycle_manager.queued_tasks)}")
    print(f"ä»»åŠ¡é˜Ÿåˆ—å¤§å°: {task_lifecycle_manager.task_queue.size()}")

    # å¯¹äºŽè¿™ç§æƒ…å†µï¼Œä»»åŠ¡åº”è¯¥è¢«è°ƒåº¦è€Œä¸æ˜¯æŽ’é˜Ÿ
    # æ³¨æ„ï¼šç”±äºŽæˆ‘ä»¬æ²¡æœ‰å®Œå…¨æ¨¡æ‹Ÿè°ƒåº¦è¿‡ç¨‹ï¼Œè¿™é‡Œçš„ç»“æžœå¯èƒ½ä¸åŒ
    print("âœ… ä»»åŠ¡å¤„ç†å®Œæˆ")

    # æµ‹è¯•åœºæ™¯3: æŒ‡å®šé›†ç¾¤èµ„æºä½¿ç”¨çŽ‡è¶…è¿‡é˜ˆå€¼
    print("\nåœºæ™¯3: æŒ‡å®šé›†ç¾¤èµ„æºä½¿ç”¨çŽ‡è¶…è¿‡é˜ˆå€¼(80%)")
    print("-" * 40)

    # ä½¿ç”¨ç›¸åŒçš„é›†ç¾¤å¿«ç…§ï¼ˆæ‰€æœ‰é›†ç¾¤éƒ½è¶…è¿‡é˜ˆå€¼ï¼‰
    cluster_monitor.get_all_cluster_info.return_value = cluster_info_over_threshold

    # åˆ›å»ºä»»åŠ¡æè¿°ï¼ŒæŒ‡å®šä½¿ç”¨cluster1
    task_desc3 = TaskDescription(
        task_id="task_preferred_cluster_1",
        name="preferred_cluster_task",
        func_or_class=lambda: None,
        args=(),
        kwargs={},
        resource_requirements={"CPU": 1.0},
        tags=["test"],
        preferred_cluster="cluster1"  # æŒ‡å®šä½¿ç”¨cluster1
    )

    # è°ƒç”¨submit_taskæ–¹æ³•
    result3 = task_lifecycle_manager.submit_task(task_desc3)

    # éªŒè¯ç»“æžœ
    print(f"ä»»åŠ¡ID: {result3}")
    print(f"é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡æ•°é‡: {len(task_lifecycle_manager.queued_tasks)}")
    print(f"ä»»åŠ¡é˜Ÿåˆ—å¤§å°: {task_lifecycle_manager.task_queue.size()}")

    # éªŒè¯ä»»åŠ¡è¢«åŠ å…¥é˜Ÿåˆ—
    assert result3 == "task_preferred_cluster_1"
    print("âœ… æŒ‡å®šé›†ç¾¤èµ„æºä¸è¶³æ—¶ä»»åŠ¡æ­£ç¡®åœ°è¢«æ”¾å…¥é˜Ÿåˆ—")

    print("\n" + "=" * 60)
    print("æ‰€æœ‰æµ‹è¯•åœºæ™¯å®Œæˆ!")
    print("=" * 60)


def demonstrate_policy_engine_logic():
    """æ¼”ç¤ºç­–ç•¥å¼•æ“Žçš„èµ„æºé˜ˆå€¼æ£€æŸ¥é€»è¾‘"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤ºç­–ç•¥å¼•æ“Žçš„èµ„æºé˜ˆå€¼æ£€æŸ¥é€»è¾‘")
    print("=" * 60)

    # åˆ›å»ºç­–ç•¥å¼•æ“Ž
    policy_engine = PolicyEngine()

    # åˆ›å»ºæµ‹è¯•æ•°æ® - æ‰€æœ‰é›†ç¾¤éƒ½è¶…è¿‡é˜ˆå€¼
    cluster_snapshots = {
        'cluster1': ResourceSnapshot(
            cluster_name='cluster1',
            total_resources={'CPU': 4.0, 'GPU': 0},
            available_resources={'CPU': 0.5, 'GPU': 0},  # ä½¿ç”¨çŽ‡87.5%
            node_count=2,
            timestamp=1234567890
        ),
        'cluster2': ResourceSnapshot(
            cluster_name='cluster2',
            total_resources={'CPU': 8.0, 'GPU': 2.0},
            available_resources={'CPU': 1.0, 'GPU': 0},  # ä½¿ç”¨çŽ‡87.5%
            node_count=3,
            timestamp=1234567890
        )
    }

    print('é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ:')
    for name, snapshot in cluster_snapshots.items():
        cpu_total = snapshot.total_resources.get('CPU', 0)
        cpu_available = snapshot.available_resources.get('CPU', 0)
        cpu_utilization = (cpu_total - cpu_available) / cpu_total if cpu_total > 0 else 0
        print(f'  {name}: CPUæ€»èµ„æº{cpu_total}, å¯ç”¨{cpu_available}, ä½¿ç”¨çŽ‡{cpu_utilization:.2%}')

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é›†ç¾¤éƒ½è¶…è¿‡é˜ˆå€¼ï¼ˆæ¨¡æ‹ŸPolicyEngineä¸­çš„é€»è¾‘ï¼‰
    all_over_threshold = True
    for name, snapshot in cluster_snapshots.items():
        cpu_total = snapshot.total_resources.get('CPU', 0)
        cpu_available = snapshot.available_resources.get('CPU', 0)
        cpu_utilization = (cpu_total - cpu_available) / cpu_total if cpu_total > 0 else 0
        if cpu_utilization <= policy_engine.RESOURCE_THRESHOLD:
            all_over_threshold = False
            print(f'  {name} çš„CPUä½¿ç”¨çŽ‡ {cpu_utilization:.2%} ä½ŽäºŽé˜ˆå€¼ {policy_engine.RESOURCE_THRESHOLD:.2%}')
        else:
            print(f'  {name} çš„CPUä½¿ç”¨çŽ‡ {cpu_utilization:.2%} è¶…è¿‡é˜ˆå€¼ {policy_engine.RESOURCE_THRESHOLD:.2%}')

    print(f'\næ‰€æœ‰é›†ç¾¤éƒ½è¶…è¿‡é˜ˆå€¼: {all_over_threshold}')

    if all_over_threshold and cluster_snapshots:
        print("âž¡ï¸  æ ¹æ®ç­–ç•¥å¼•æ“Žé€»è¾‘ï¼Œä»»åŠ¡å°†è¢«æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…")
    else:
        print("âž¡ï¸  æ ¹æ®ç­–ç•¥å¼•æ“Žé€»è¾‘ï¼Œä»»åŠ¡å°†è¢«è°ƒåº¦åˆ°å¯ç”¨é›†ç¾¤")

    print("\n" + "=" * 60)


if __name__ == "__main__":
    # æ¼”ç¤ºç­–ç•¥å¼•æ“Žçš„é€»è¾‘
    demonstrate_policy_engine_logic()

    # æµ‹è¯•èµ„æºé˜ˆå€¼é˜Ÿåˆ—åŠŸèƒ½
    test_resource_threshold_queue_functionality()

    print("\nðŸŽ‰ æ‰€æœ‰æµ‹è¯•å’Œæ¼”ç¤ºå®Œæˆ!")