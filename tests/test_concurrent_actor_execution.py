#!/usr/bin/env python3
"""
å¹¶å‘Actoræ‰§è¡Œæµ‹è¯•ç”¨ä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨submit_actorç»Ÿä¸€è°ƒåº¦æ¥å£å®ç°10ä¸ªå¹¶å‘ä»»åŠ¡æ‰§è¡Œ
"""

import sys
import os
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/..')

import ray
from ray import actor
from ray_multicluster_scheduler.app.client_api.unified_scheduler import (
    UnifiedScheduler,
    initialize_scheduler_environment,
    submit_actor,
    submit_task
)


# å®šä¹‰ä¸€ä¸ªç®€å•çš„Actorç±»ç”¨äºæµ‹è¯•
@ray.remote
class TestActor:
    """æµ‹è¯•Actorç±»"""

    def __init__(self, actor_id):
        self.actor_id = actor_id
        self.execution_count = 0

    def process_task(self, task_data):
        """å¤„ç†ä»»åŠ¡æ–¹æ³•"""
        # æ¨¡æ‹Ÿä»»åŠ¡å¤„ç†æ—¶é—´
        time.sleep(0.5)
        self.execution_count += 1
        result = f"Actor {self.actor_id} processed task {task_data} (count: {self.execution_count})"
        print(result)
        return result

    def get_execution_count(self):
        """è·å–æ‰§è¡Œæ¬¡æ•°"""
        return self.execution_count

    def get_actor_info(self):
        """è·å–Actorä¿¡æ¯"""
        return f"Actor {self.actor_id}"


def test_concurrent_actor_execution():
    """æµ‹è¯•å¹¶å‘Actoræ‰§è¡Œ"""
    print("=" * 60)
    print("æµ‹è¯•å¹¶å‘Actoræ‰§è¡Œ")
    print("=" * 60)

    try:
        # 1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ
        print("1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ...")
        task_lifecycle_manager = initialize_scheduler_environment()
        print("âœ… è°ƒåº¦å™¨ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")

        # 2. åˆ›å»º10ä¸ªå¹¶å‘Actor
        print("\n2. åˆ›å»º10ä¸ªå¹¶å‘Actor...")
        actors = []
        actor_futures = []

        for i in range(10):
            print(f"  åˆ›å»ºç¬¬ {i+1} ä¸ªActor...")
            actor_id, actor_instance = submit_actor(
                actor_class=TestActor,
                args=(f"Actor-{i}",),
                kwargs={},
                resource_requirements={"CPU": 0.5},  # æ¯ä¸ªActoréœ€è¦0.5ä¸ªCPUæ ¸å¿ƒ
                tags=["test", "concurrent"],
                name=f"ConcurrentTestActor-{i}",
                preferred_cluster=None  # è®©è°ƒåº¦å™¨è‡ªåŠ¨é€‰æ‹©é›†ç¾¤
            )

            actors.append((actor_id, actor_instance))
            actor_futures.append(actor_instance)
            print(f"  âœ… Actor {i} åˆ›å»ºæˆåŠŸï¼ŒID: {actor_id}")

        print(f"\nâœ… æˆåŠŸåˆ›å»º {len(actors)} ä¸ªActor")

        # 3. å¹¶å‘æ‰§è¡Œä»»åŠ¡
        print("\n3. å¹¶å‘æ‰§è¡Œä»»åŠ¡...")
        task_results = []

        # ä¸ºæ¯ä¸ªActoræäº¤å¤šä¸ªä»»åŠ¡
        for i, (actor_id, actor_instance) in enumerate(actors):
            for j in range(3):  # æ¯ä¸ªActoræ‰§è¡Œ3ä¸ªä»»åŠ¡
                print(f"  ä¸ºActor {i} æäº¤ä»»åŠ¡ {j+1}...")
                # ä½¿ç”¨Actorçš„æ–¹æ³•æ‰§è¡Œä»»åŠ¡
                result = actor_instance.process_task.remote(f"Task-{i}-{j}")
                task_results.append((f"Actor-{i}-Task-{j}", result))

        print(f"\nâœ… å·²æäº¤ {len(task_results)} ä¸ªä»»åŠ¡")

        # 4. æ”¶é›†ä»»åŠ¡ç»“æœ
        print("\n4. æ”¶é›†ä»»åŠ¡ç»“æœ...")
        completed_tasks = 0

        for task_name, future in task_results:
            try:
                result = ray.get(future, timeout=10)  # è®¾ç½®10ç§’è¶…æ—¶
                print(f"  âœ… {task_name} å®Œæˆ: {result}")
                completed_tasks += 1
            except Exception as e:
                print(f"  âŒ {task_name} å¤±è´¥: {e}")

        print(f"\nâœ… æˆåŠŸå®Œæˆ {completed_tasks}/{len(task_results)} ä¸ªä»»åŠ¡")

        # 5. éªŒè¯ActorçŠ¶æ€
        print("\n5. éªŒè¯ActorçŠ¶æ€...")
        for i, (actor_id, actor_instance) in enumerate(actors):
            try:
                execution_count = ray.get(actor_instance.get_execution_count.remote(), timeout=5)
                actor_info = ray.get(actor_instance.get_actor_info.remote(), timeout=5)
                print(f"  Actor {i}: {actor_info}, æ‰§è¡Œæ¬¡æ•°: {execution_count}")
            except Exception as e:
                print(f"  âŒ æ— æ³•è·å–Actor {i} çŠ¶æ€: {e}")

        # 6. æ¸…ç†èµ„æº
        print("\n6. æ¸…ç†èµ„æº...")
        if task_lifecycle_manager:
            task_lifecycle_manager.stop()
            print("âœ… ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨å·²åœæ­¢")

        print("\nğŸ‰ å¹¶å‘Actoræ‰§è¡Œæµ‹è¯•å®Œæˆ!")
        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

        # å°è¯•æ¸…ç†èµ„æº
        try:
            from ray_multicluster_scheduler.app.client_api.submit_actor import _task_lifecycle_manager
            if _task_lifecycle_manager:
                _task_lifecycle_manager.stop()
                print("âœ… ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨å·²åœæ­¢")
        except:
            pass

        return False


def test_concurrent_actor_with_threadpool():
    """ä½¿ç”¨çº¿ç¨‹æ± æµ‹è¯•å¹¶å‘Actoræ‰§è¡Œ"""
    print("\n" + "=" * 60)
    print("ä½¿ç”¨çº¿ç¨‹æ± æµ‹è¯•å¹¶å‘Actoræ‰§è¡Œ")
    print("=" * 60)

    try:
        # 1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ
        print("1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ...")
        task_lifecycle_manager = initialize_scheduler_environment()
        print("âœ… è°ƒåº¦å™¨ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")

        # 2. ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ›å»ºActor
        print("\n2. ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ›å»ºActor...")
        actors = []

        def create_actor(i):
            """åˆ›å»ºå•ä¸ªActor"""
            actor_id, actor_instance = submit_actor(
                actor_class=TestActor,
                args=(f"ThreadPoolActor-{i}",),
                kwargs={},
                resource_requirements={"CPU": 0.3},
                tags=["test", "threadpool"],
                name=f"ThreadPoolActor-{i}",
                preferred_cluster=None
            )
            return i, actor_id, actor_instance

        # ä½¿ç”¨ThreadPoolExecutorå¹¶å‘åˆ›å»º10ä¸ªActor
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤æ‰€æœ‰åˆ›å»ºActorçš„ä»»åŠ¡
            future_to_index = {executor.submit(create_actor, i): i for i in range(10)}

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                try:
                    i, actor_id, actor_instance = future.result(timeout=30)
                    actors.append((actor_id, actor_instance))
                    print(f"  âœ… Actor {i} åˆ›å»ºæˆåŠŸï¼ŒID: {actor_id}")
                except Exception as e:
                    index = future_to_index[future]
                    print(f"  âŒ Actor {index} åˆ›å»ºå¤±è´¥: {e}")

        print(f"\nâœ… æˆåŠŸåˆ›å»º {len(actors)} ä¸ªActor")

        # 3. å¹¶å‘æ‰§è¡Œä»»åŠ¡
        print("\n3. å¹¶å‘æ‰§è¡Œä»»åŠ¡...")
        all_results = []

        def execute_actor_task(actor_index, actor_instance, task_index):
            """æ‰§è¡ŒActorä»»åŠ¡"""
            result = actor_instance.process_task.remote(f"ThreadPoolTask-{actor_index}-{task_index}")
            return actor_index, task_index, result

        # ä½¿ç”¨ThreadPoolExecutorå¹¶å‘æ‰§è¡Œä»»åŠ¡
        with ThreadPoolExecutor(max_workers=10) as executor:
            # ä¸ºæ¯ä¸ªActoræäº¤2ä¸ªä»»åŠ¡
            futures = []
            for i, (actor_id, actor_instance) in enumerate(actors):
                for j in range(2):
                    future = executor.submit(execute_actor_task, i, actor_instance, j)
                    futures.append(future)

            # æ”¶é›†ä»»åŠ¡ç»“æœ
            for future in as_completed(futures):
                try:
                    actor_index, task_index, result_future = future.result(timeout=30)
                    result = ray.get(result_future, timeout=10)
                    all_results.append(result)
                    print(f"  âœ… Actor {actor_index} ä»»åŠ¡ {task_index} å®Œæˆ")
                except Exception as e:
                    print(f"  âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

        print(f"\nâœ… æˆåŠŸæ‰§è¡Œ {len(all_results)} ä¸ªä»»åŠ¡")

        # 4. æ¸…ç†èµ„æº
        print("\n4. æ¸…ç†èµ„æº...")
        if task_lifecycle_manager:
            task_lifecycle_manager.stop()
            print("âœ… ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨å·²åœæ­¢")

        print("\nğŸ‰ çº¿ç¨‹æ± å¹¶å‘Actoræ‰§è¡Œæµ‹è¯•å®Œæˆ!")
        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False


def demonstrate_actor_load_distribution():
    """æ¼”ç¤ºActorè´Ÿè½½åˆ†å¸ƒ"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤ºActorè´Ÿè½½åˆ†å¸ƒ")
    print("=" * 60)

    try:
        # 1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ
        print("1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ...")
        task_lifecycle_manager = initialize_scheduler_environment()
        print("âœ… è°ƒåº¦å™¨ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")

        # 2. è·å–é›†ç¾¤ä¿¡æ¯
        cluster_monitor = task_lifecycle_manager.cluster_monitor
        cluster_monitor.refresh_resource_snapshots(force=True)
        cluster_info = cluster_monitor.get_all_cluster_info()

        print(f"\nå‘ç° {len(cluster_info)} ä¸ªé›†ç¾¤:")
        for name, info in cluster_info.items():
            metadata = info['metadata']
            snapshot = info['snapshot']
            print(f"  é›†ç¾¤ [{name}]: åå¥½={metadata.prefer}, åœ°å€={metadata.head_address}")
            if snapshot:
                cpu_free = snapshot.available_resources.get("CPU", 0)
                cpu_total = snapshot.total_resources.get("CPU", 0)
                print(f"    CPUèµ„æº: {cpu_free}/{cpu_total}")

        # 3. åˆ›å»ºå¤šä¸ªActorè§‚å¯Ÿåˆ†å¸ƒæƒ…å†µ
        print("\n2. åˆ›å»ºå¤šä¸ªActorè§‚å¯Ÿåˆ†å¸ƒæƒ…å†µ...")
        actors = []

        for i in range(8):
            actor_id, actor_instance = submit_actor(
                actor_class=TestActor,
                args=(f"DistributedActor-{i}",),
                kwargs={},
                resource_requirements={"CPU": 0.5},
                tags=["test", "distribution"],
                name=f"DistributedActor-{i}",
                preferred_cluster=None  # è®©è°ƒåº¦å™¨è‡ªåŠ¨åˆ†é…
            )
            actors.append((actor_id, actor_instance))
            print(f"  âœ… DistributedActor-{i} åˆ›å»ºæˆåŠŸ")
            time.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿä»¥è§‚å¯Ÿè°ƒåº¦è¡Œä¸º

        # 4. æ‰§è¡Œä»»åŠ¡éªŒè¯
        print("\n3. æ‰§è¡Œä»»åŠ¡éªŒè¯...")
        results = []
        for i, (actor_id, actor_instance) in enumerate(actors[:5]):  # åªæµ‹è¯•å‰5ä¸ª
            result = actor_instance.get_actor_info.remote()
            results.append(ray.get(result, timeout=5))

        print("Actorä¿¡æ¯:")
        for result in results:
            print(f"  {result}")

        # 5. æ¸…ç†èµ„æº
        print("\n4. æ¸…ç†èµ„æº...")
        if task_lifecycle_manager:
            task_lifecycle_manager.stop()
            print("âœ… ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨å·²åœæ­¢")

        print("\nğŸ‰ Actorè´Ÿè½½åˆ†å¸ƒæ¼”ç¤ºå®Œæˆ!")
        return True

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # è¿è¡Œå¹¶å‘Actoræ‰§è¡Œæµ‹è¯•
    success1 = test_concurrent_actor_execution()

    # è¿è¡Œçº¿ç¨‹æ± å¹¶å‘Actoræ‰§è¡Œæµ‹è¯•
    success2 = test_concurrent_actor_with_threadpool()

    # è¿è¡ŒActorè´Ÿè½½åˆ†å¸ƒæ¼”ç¤º
    success3 = demonstrate_actor_load_distribution()

    print("\n" + "=" * 60)
    if success1 and success2 and success3:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
    print("=" * 60)