#!/usr/bin/env python3
"""
çœŸå®é›†ç¾¤è¿æ¥è·¨é›†ç¾¤è°ƒåº¦æµ‹è¯•ç”¨ä¾‹
ä½¿ç”¨çœŸå®çš„é›†ç¾¤è¿æ¥æ¥éªŒè¯è·¨é›†ç¾¤è°ƒåº¦æœºåˆ¶
"""

import sys
import os
import time
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/..')

from ray_multicluster_scheduler.app.client_api.unified_scheduler import (
    UnifiedScheduler, 
    initialize_scheduler_environment, 
    submit_task
)


def simple_test_task(task_id, task_name, duration=1):
    """ç®€å•çš„æµ‹è¯•ä»»åŠ¡å‡½æ•°"""
    import time
    print(f"ä»»åŠ¡ {task_id} ({task_name}) å¼€å§‹æ‰§è¡Œ")
    time.sleep(duration)
    result = f"ä»»åŠ¡ {task_id} ({task_name}) æ‰§è¡Œå®Œæˆ"
    print(result)
    return result


def test_cross_cluster_scheduling_with_real_clusters():
    """ä½¿ç”¨çœŸå®é›†ç¾¤è¿æ¥æµ‹è¯•è·¨é›†ç¾¤è°ƒåº¦"""
    print("=" * 70)
    print("ä½¿ç”¨çœŸå®é›†ç¾¤è¿æ¥æµ‹è¯•è·¨é›†ç¾¤è°ƒåº¦")
    print("=" * 70)
    
    try:
        # 1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ
        print("1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ...")
        task_lifecycle_manager = initialize_scheduler_environment()
        print("âœ… è°ƒåº¦å™¨ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
        
        # 2. æ˜¾ç¤ºé›†ç¾¤ä¿¡æ¯
        print("\n2. é›†ç¾¤ä¿¡æ¯:")
        cluster_monitor = task_lifecycle_manager.cluster_monitor
        cluster_monitor.refresh_resource_snapshots(force=True)
        cluster_info = cluster_monitor.get_all_cluster_info()
        
        for name, info in cluster_info.items():
            metadata = info['metadata']
            snapshot = info['snapshot']
            print(f"  é›†ç¾¤ [{name}]:")
            print(f"    åœ°å€: {metadata.head_address}")
            print(f"    æ˜¯å¦åå¥½é›†ç¾¤: {'æ˜¯' if metadata.prefer else 'å¦'}")
            if snapshot:
                cpu_free = snapshot.available_resources.get("CPU", 0)
                cpu_total = snapshot.total_resources.get("CPU", 0)
                gpu_free = snapshot.available_resources.get("GPU", 0)
                gpu_total = snapshot.total_resources.get("GPU", 0)
                
                cpu_utilization = (cpu_total - cpu_free) / cpu_total if cpu_total > 0 else 0
                gpu_utilization = (gpu_total - gpu_free) / gpu_total if gpu_total > 0 else 0
                
                print(f"    CPU: {cpu_free}/{cpu_total} (ä½¿ç”¨ç‡: {cpu_utilization:.1%})")
                print(f"    GPU: {gpu_free}/{gpu_total} (ä½¿ç”¨ç‡: {gpu_utilization:.1%})")
            else:
                print("    âŒ æ— æ³•è·å–èµ„æºä¿¡æ¯")
        
        # 3. æäº¤ä»»åŠ¡åˆ°é¦–é€‰é›†ç¾¤ï¼ˆcentosï¼‰
        print("\n3. æäº¤ä»»åŠ¡åˆ°é¦–é€‰é›†ç¾¤ï¼ˆcentosï¼‰...")
        
        # æäº¤å‡ ä¸ªä»»åŠ¡åˆ°centosé›†ç¾¤
        results = []
        for i in range(3):
            task_id, result = submit_task(
                func=simple_test_task,
                args=(f"centos-task-{i}", f"CentOSä»»åŠ¡{i}", 1),
                kwargs={},
                resource_requirements={"CPU": 1.0},
                tags=["test", "centos"],
                name=f"centos_test_task_{i}",
                preferred_cluster="centos"
            )
            results.append((task_id, result))
            print(f"  âœ… ä»»åŠ¡æäº¤æˆåŠŸ: {task_id}")
        
        # 4. æäº¤ä»»åŠ¡åˆ°macé›†ç¾¤
        print("\n4. æäº¤ä»»åŠ¡åˆ°macé›†ç¾¤...")
        
        for i in range(2):
            task_id, result = submit_task(
                func=simple_test_task,
                args=(f"mac-task-{i}", f"Macä»»åŠ¡{i}", 1),
                kwargs={},
                resource_requirements={"CPU": 1.0},
                tags=["test", "mac"],
                name=f"mac_test_task_{i}",
                preferred_cluster="mac"
            )
            results.append((task_id, result))
            print(f"  âœ… ä»»åŠ¡æäº¤æˆåŠŸ: {task_id}")
        
        # 5. æäº¤ä¸æŒ‡å®šé›†ç¾¤çš„ä»»åŠ¡ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
        print("\n5. æäº¤ä¸æŒ‡å®šé›†ç¾¤çš„ä»»åŠ¡ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰...")
        
        for i in range(2):
            task_id, result = submit_task(
                func=simple_test_task,
                args=(f"balanced-task-{i}", f"è´Ÿè½½å‡è¡¡ä»»åŠ¡{i}", 1),
                kwargs={},
                resource_requirements={"CPU": 1.0},
                tags=["test", "balanced"],
                name=f"balanced_test_task_{i}"
                # ä¸æŒ‡å®špreferred_clusterï¼Œä½¿ç”¨è´Ÿè½½å‡è¡¡
            )
            results.append((task_id, result))
            print(f"  âœ… ä»»åŠ¡æäº¤æˆåŠŸ: {task_id}")
        
        # 6. éªŒè¯ç»“æœ
        print("\n6. éªŒè¯ç»“æœ...")
        for task_id, result in results:
            if result:
                print(f"  âœ… ä»»åŠ¡ {task_id} æ‰§è¡ŒæˆåŠŸ")
            else:
                print(f"  âš ï¸  ä»»åŠ¡ {task_id} ç»“æœä¸ºç©º")
        
        # 7. æ¸…ç†èµ„æº
        print("\n7. æ¸…ç†èµ„æº...")
        if task_lifecycle_manager:
            task_lifecycle_manager.stop()
            print("âœ… ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨å·²åœæ­¢")
        
        print("\nğŸ‰ çœŸå®é›†ç¾¤è·¨é›†ç¾¤è°ƒåº¦æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        
        # å°è¯•æ¸…ç†èµ„æº
        try:
            from ray_multicluster_scheduler.app.client_api.submit_task import _task_lifecycle_manager
            if _task_lifecycle_manager:
                _task_lifecycle_manager.stop()
                print("âœ… ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨å·²åœæ­¢")
        except:
            pass
            
        return False


def test_scenario_exceeding_cluster_capacity():
    """æµ‹è¯•è¶…å‡ºé›†ç¾¤å®¹é‡çš„åœºæ™¯"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•è¶…å‡ºé›†ç¾¤å®¹é‡çš„åœºæ™¯")
    print("=" * 70)
    
    try:
        # 1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ
        print("1. åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ...")
        task_lifecycle_manager = initialize_scheduler_environment()
        print("âœ… è°ƒåº¦å™¨ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
        
        # 2. æŸ¥çœ‹å½“å‰é›†ç¾¤èµ„æºæƒ…å†µ
        print("\n2. å½“å‰é›†ç¾¤èµ„æºæƒ…å†µ:")
        cluster_monitor = task_lifecycle_manager.cluster_monitor
        cluster_monitor.refresh_resource_snapshots(force=True)
        cluster_info = cluster_monitor.get_all_cluster_info()
        
        total_capacity = 0
        for name, info in cluster_info.items():
            metadata = info['metadata']
            snapshot = info['snapshot']
            print(f"  é›†ç¾¤ [{name}]:")
            if snapshot:
                cpu_free = snapshot.available_resources.get("CPU", 0)
                cpu_total = snapshot.total_resources.get("CPU", 0)
                total_capacity += cpu_total
                print(f"    CPUæ€»å®¹é‡: {cpu_total}, å¯ç”¨: {cpu_free}")
            else:
                print("    âŒ æ— æ³•è·å–èµ„æºä¿¡æ¯")
        
        print(f"\n  æ€»é›†ç¾¤CPUå®¹é‡: {total_capacity}")
        
        # 3. æäº¤è¶…è¿‡é›†ç¾¤æ€»å®¹é‡çš„ä»»åŠ¡æ¥æµ‹è¯•æ’é˜Ÿæœºåˆ¶
        print(f"\n3. æäº¤ {int(total_capacity) + 5} ä¸ªä»»åŠ¡æ¥æµ‹è¯•æ’é˜Ÿæœºåˆ¶...")
        
        task_results = []
        for i in range(int(total_capacity) + 5):
            try:
                task_id, result = submit_task(
                    func=simple_test_task,
                    args=(f"overflow-task-{i}", f"æº¢å‡ºä»»åŠ¡{i}", 2),
                    kwargs={},
                    resource_requirements={"CPU": 1.0},
                    tags=["test", "overflow"],
                    name=f"overflow_test_task_{i}"
                )
                task_results.append((task_id, result))
                print(f"  âœ… ä»»åŠ¡ {task_id} æäº¤æˆåŠŸ")
            except Exception as e:
                print(f"  âŒ ä»»åŠ¡æäº¤å¤±è´¥: {e}")
        
        # 4. éªŒè¯éƒ¨åˆ†ä»»åŠ¡è¢«æ‰§è¡Œï¼Œéƒ¨åˆ†ä»»åŠ¡å¯èƒ½æ’é˜Ÿ
        print("\n4. ä»»åŠ¡æ‰§è¡Œæƒ…å†µ:")
        successful_tasks = 0
        for task_id, result in task_results:
            if result:
                successful_tasks += 1
                print(f"  âœ… ä»»åŠ¡ {task_id} æ‰§è¡ŒæˆåŠŸ")
            else:
                print(f"  âš ï¸  ä»»åŠ¡ {task_id} ç»“æœä¸ºç©º")
        
        print(f"\n  æˆåŠŸæ‰§è¡Œçš„ä»»åŠ¡æ•°: {successful_tasks}/{len(task_results)}")
        
        # 5. æ¸…ç†èµ„æº
        print("\n5. æ¸…ç†èµ„æº...")
        if task_lifecycle_manager:
            task_lifecycle_manager.stop()
            print("âœ… ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨å·²åœæ­¢")
        
        print("\nğŸ‰ è¶…å‡ºé›†ç¾¤å®¹é‡åœºæ™¯æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False


def demonstrate_cross_cluster_behavior():
    """æ¼”ç¤ºè·¨é›†ç¾¤è¡Œä¸º"""
    print("\n" + "=" * 70)
    print("è·¨é›†ç¾¤è°ƒåº¦è¡Œä¸ºæ¼”ç¤º")
    print("=" * 70)
    
    print("\nç³»ç»Ÿè·¨é›†ç¾¤è°ƒåº¦æœºåˆ¶è¯´æ˜:")
    print("1. é¦–é€‰é›†ç¾¤ä¼˜å…ˆ: å¦‚æœç”¨æˆ·æŒ‡å®šäº†preferred_clusterï¼Œç³»ç»Ÿä¼šä¼˜å…ˆå°è¯•è°ƒåº¦åˆ°è¯¥é›†ç¾¤")
    print("2. èµ„æºé˜ˆå€¼æ§åˆ¶: å½“é›†ç¾¤èµ„æºä½¿ç”¨ç‡è¶…è¿‡80%æ—¶ï¼Œæ–°ä»»åŠ¡ä¼šè¢«æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…")
    print("3. è´Ÿè½½å‡è¡¡: æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶ï¼Œç³»ç»Ÿä¼šé€‰æ‹©èµ„æºæœ€å……è¶³çš„é›†ç¾¤")
    print("4. åŠ¨æ€é‡è°ƒåº¦: ç³»ç»Ÿæ¯30ç§’ä¼šé‡æ–°è¯„ä¼°é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ï¼Œå°è¯•å°†å…¶è°ƒåº¦åˆ°åˆé€‚çš„é›†ç¾¤")
    print("5. ä»»åŠ¡é˜Ÿåˆ—: æ— æ³•ç«‹å³è°ƒåº¦çš„ä»»åŠ¡ä¼šè¢«ä¿å­˜åœ¨é˜Ÿåˆ—ä¸­ï¼Œç›´åˆ°æœ‰åˆé€‚èµ„æº")
    
    print("\næµ‹è¯•åœºæ™¯æ€»ç»“:")
    print("âœ“ ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®é›†ç¾¤èµ„æºæƒ…å†µæ™ºèƒ½è°ƒåº¦ä»»åŠ¡")
    print("âœ“ æŒ‡å®šé›†ç¾¤çš„ä»»åŠ¡ä¼šä¼˜å…ˆè°ƒåº¦åˆ°æŒ‡å®šé›†ç¾¤")
    print("âœ“ æœªæŒ‡å®šé›†ç¾¤çš„ä»»åŠ¡ä¼šæ ¹æ®è´Ÿè½½å‡è¡¡ç®—æ³•è°ƒåº¦")
    print("âœ“ èµ„æºç´§å¼ æ—¶ä»»åŠ¡ä¼šè¿›å…¥é˜Ÿåˆ—ç­‰å¾…")
    print("âœ“ èµ„æºé‡Šæ”¾åé˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ä¼šè¢«é‡æ–°è°ƒåº¦")


if __name__ == "__main__":
    # è¿è¡ŒçœŸå®é›†ç¾¤è·¨é›†ç¾¤è°ƒåº¦æµ‹è¯•
    success1 = test_cross_cluster_scheduling_with_real_clusters()
    
    # è¿è¡Œè¶…å‡ºé›†ç¾¤å®¹é‡åœºæ™¯æµ‹è¯•
    success2 = test_scenario_exceeding_cluster_capacity()
    
    # æ¼”ç¤ºè·¨é›†ç¾¤è¡Œä¸º
    demonstrate_cross_cluster_behavior()
    
    print("\n" + "=" * 70)
    if success1 and success2:
        print("ğŸ‰ æ‰€æœ‰çœŸå®é›†ç¾¤æµ‹è¯•é€šè¿‡!")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
    print("=" * 70)