#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆå¤šé›†ç¾¤å¹¶å‘è°ƒåº¦æµ‹è¯•ç”¨ä¾‹
ç›´æ¥ä½¿ç”¨é›†ç¾¤ç®¡ç†å™¨çš„è¯„åˆ†æœºåˆ¶éªŒè¯è·¨é›†ç¾¤è´Ÿè½½å‡è¡¡
"""

import sys
import os
import time
from collections import defaultdict
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/..')

from ray_multicluster_scheduler.common.model import TaskDescription, ResourceSnapshot, ClusterMetadata
from ray_multicluster_scheduler.scheduler.cluster.cluster_manager import ClusterManager
from ray_multicluster_scheduler.scheduler.policy.policy_engine import PolicyEngine


def final_multi_cluster_concurrency_test():
    """æœ€ç»ˆç‰ˆå¤šé›†ç¾¤å¹¶å‘è°ƒåº¦æµ‹è¯•"""
    print("=" * 80)
    print("ğŸ” æœ€ç»ˆç‰ˆå¤šé›†ç¾¤å¹¶å‘è°ƒåº¦æµ‹è¯•")
    print("=" * 80)

    # åˆ›å»ºé›†ç¾¤ç®¡ç†å™¨
    cluster_manager = ClusterManager()

    # æ‰‹åŠ¨æ·»åŠ é›†ç¾¤é…ç½®
    cluster_configs = {
        "centos": ClusterMetadata(
            name="centos",
            head_address="192.168.5.7:32546",
            dashboard="http://192.168.5.7:31591",
            prefer=False,
            weight=1.0,
            runtime_env={
                "conda": "ts",
                "env_vars": {"home_dir": "/home/zorro"}
            },
            tags=["linux", "x86_64"]
        ),
        "mac": ClusterMetadata(
            name="mac",
            head_address="192.168.5.2:32546",
            dashboard="http://192.168.5.2:8265",
            prefer=True,
            weight=1.2,
            runtime_env={
                "conda": "k8s",
                "env_vars": {"home_dir": "/Users/zorro"}
            },
            tags=["macos", "arm64"]
        )
    }

    # å°†é›†ç¾¤é…ç½®æ·»åŠ åˆ°é›†ç¾¤ç®¡ç†å™¨
    for name, config in cluster_configs.items():
        cluster_manager.clusters[name] = config

    # æ¨¡æ‹Ÿé›†ç¾¤å¥åº·çŠ¶æ€å’Œèµ„æºå¿«ç…§
    current_time = time.time()

    # ä¸ºcentosé›†ç¾¤åˆ›å»ºå¥åº·çŠ¶æ€
    from ray_multicluster_scheduler.scheduler.cluster.cluster_manager import ClusterHealth
    centos_health = ClusterHealth()
    centos_resources = {
        "available": {"CPU": 16.0, "GPU": 0},
        "total": {"CPU": 16.0, "GPU": 0},
        "cpu_free": 16.0,
        "cpu_total": 16.0,
        "gpu_free": 0,
        "gpu_total": 0,
        "cpu_utilization": 0.0,
        "node_count": 2
    }
    centos_health.update(16.0, centos_resources, True)  # è¯„åˆ†ä¸º16.0
    cluster_manager.health_status["centos"] = centos_health

    # ä¸ºmacé›†ç¾¤åˆ›å»ºå¥åº·çŠ¶æ€
    mac_health = ClusterHealth()
    mac_resources = {
        "available": {"CPU": 8.0, "GPU": 0, "MacCPU": 8.0},
        "total": {"CPU": 8.0, "GPU": 0, "MacCPU": 8.0},
        "cpu_free": 8.0,
        "cpu_total": 8.0,
        "gpu_free": 0,
        "gpu_total": 0,
        "cpu_utilization": 0.0,
        "node_count": 1
    }
    mac_health.update(11.52, mac_resources, True)  # è¯„åˆ†ä¸º11.52 (8.0 * 1.2 * 1.2)
    cluster_manager.health_status["mac"] = mac_health

    # ç»Ÿè®¡å˜é‡
    cluster_distribution = defaultdict(int)

    # æäº¤30ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éœ€è¦2ä¸ªCPUï¼Œä¸æŒ‡å®šé›†ç¾¤
    print(f"\nğŸš€ æäº¤30ä¸ªä»»åŠ¡ï¼ˆæ¯ä¸ªä»»åŠ¡éœ€è¦2ä¸ªCPUï¼Œä¸æŒ‡å®šé›†ç¾¤ï¼‰:")
    print(f"   â€¢ centosé›†ç¾¤: 16ä¸ªCPU (è¯„åˆ†: 16.0)")
    print(f"   â€¢ macé›†ç¾¤: 8ä¸ªCPU (è¯„åˆ†: 11.52)")
    print(f"   â€¢ æ€»å¯ç”¨CPU: 24ä¸ª")
    print(f"   â€¢ æ€»éœ€æ±‚CPU: 60ä¸ª (30ä¸ªä»»åŠ¡ Ã— 2ä¸ªCPU)")

    # ä½¿ç”¨é›†ç¾¤ç®¡ç†å™¨çš„select_best_clusteræ–¹æ³•è¿›è¡Œè°ƒåº¦
    for i in range(30):
        requirements = {"CPU": 2.0}
        best_cluster = cluster_manager.select_best_cluster(requirements)

        if best_cluster:
            cluster_distribution[best_cluster] += 1
            # æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦åæ›´æ–°é›†ç¾¤èµ„æº
            health = cluster_manager.health_status[best_cluster]
            resources = health.resources
            cpu_free = resources.get("cpu_free", 0)

            # æ›´æ–°å¯ç”¨èµ„æºï¼ˆæ¨¡æ‹Ÿä»»åŠ¡å ç”¨ï¼‰
            new_cpu_free = max(0, cpu_free - 2.0)
            resources["cpu_free"] = new_cpu_free
            resources["available"]["CPU"] = new_cpu_free

            # é‡æ–°è®¡ç®—è¯„åˆ†
            config = cluster_configs[best_cluster]
            cpu_total = resources.get("cpu_total", 0)
            gpu_free = resources.get("gpu_free", 0)
            cpu_utilization = (cpu_total - new_cpu_free) / cpu_total if cpu_total > 0 else 0

            # é‡æ–°è®¡ç®—è¯„åˆ†
            base_score = new_cpu_free * config.weight
            gpu_bonus = gpu_free * 5
            preference_bonus = 1.2 if config.prefer else 1.0
            load_factor = 1.0 - cpu_utilization
            new_score = (base_score + gpu_bonus) * preference_bonus * load_factor

            # æ›´æ–°å¥åº·çŠ¶æ€
            health.score = new_score
            health.resources = resources

            print(f"    ä»»åŠ¡ {i}: è°ƒåº¦åˆ° {best_cluster} (å‰©ä½™CPU: {new_cpu_free:.1f}, æ–°è¯„åˆ†: {new_score:.2f})")
        else:
            print(f"    ä»»åŠ¡ {i}: æ— åˆé€‚é›†ç¾¤ï¼Œè¿›å…¥é˜Ÿåˆ—")

    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
    generate_final_test_report(cluster_distribution)

    return cluster_distribution


def generate_final_test_report(cluster_distribution):
    """ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š"""
    print(f"\nğŸ“‹ é›†ç¾¤åˆ†å¸ƒç»Ÿè®¡:")
    total_scheduled = sum(cluster_distribution.values())

    for cluster_name, count in cluster_distribution.items():
        print(f"  â€¢ {cluster_name}: {count}ä¸ªä»»åŠ¡")

    print(f"\nğŸ“‹ è°ƒåº¦è¡Œä¸ºåˆ†æ:")
    if len(cluster_distribution) > 1:
        print(f"  âœ… ç³»ç»Ÿèƒ½å¤Ÿå°†ä»»åŠ¡åˆ†æ•£åˆ°å¤šä¸ªé›†ç¾¤è¿›è¡Œè°ƒåº¦")
        print(f"     â€¢ ä¸åŒé›†ç¾¤éƒ½æœ‰ä»»åŠ¡è¢«è°ƒåº¦")
        print(f"     â€¢ å®ç°äº†è·¨é›†ç¾¤è´Ÿè½½å‡è¡¡")

        # è®¡ç®—è´Ÿè½½å‡è¡¡ç¨‹åº¦
        counts = list(cluster_distribution.values())
        max_count = max(counts)
        min_count = min(counts)
        balance_ratio = min_count / max_count if max_count > 0 else 0

        print(f"     â€¢ è´Ÿè½½å‡è¡¡æ¯”ç‡: {balance_ratio:.2f} (è¶Šæ¥è¿‘1è¶Šå‡è¡¡)")
    else:
        print(f"  âš ï¸  ç³»ç»Ÿåªåœ¨ä¸€ä¸ªé›†ç¾¤ä¸Šè¿›è¡Œè°ƒåº¦")
        print(f"     â€¢ åªæœ‰ä¸€ä¸ªé›†ç¾¤æœ‰ä»»åŠ¡è¢«è°ƒåº¦")
        print(f"     â€¢ æœªå®ç°è·¨é›†ç¾¤è´Ÿè½½å‡è¡¡")

    print(f"\nğŸ“ˆ è°ƒåº¦ç»Ÿè®¡:")
    print(f"   â€¢ æ€»è°ƒåº¦ä»»åŠ¡: {total_scheduled}ä¸ª")


def compare_scheduling_approaches():
    """æ¯”è¾ƒä¸åŒçš„è°ƒåº¦æ–¹æ³•"""
    print("\n" + "=" * 80)
    print("ğŸ”„ è°ƒåº¦æ–¹æ³•å¯¹æ¯”åˆ†æ")
    print("=" * 80)

    print(f"\nğŸ“‹ ç®€åŒ–è¯„åˆ†ç­–ç•¥ vs çœŸå®é›†ç¾¤è¯„åˆ†:")
    print(f"  ç®€åŒ–è¯„åˆ†ç­–ç•¥é—®é¢˜:")
    print(f"    â€¢ ä½¿ç”¨å›ºå®šå€¼å½’ä¸€åŒ–å¯¼è‡´è¯„åˆ†å¤±çœŸ")
    print(f"    â€¢ æœªè€ƒè™‘é›†ç¾¤æƒé‡å’Œåå¥½è®¾ç½®")
    print(f"    â€¢ æ— æ³•æ­£ç¡®åæ˜ é›†ç¾¤å®é™…è´Ÿè½½èƒ½åŠ›")

    print(f"\n  çœŸå®é›†ç¾¤è¯„åˆ†ä¼˜åŠ¿:")
    print(f"    â€¢ åŸºäºå®é™…èµ„æºé…ç½®è¿›è¡Œè¯„åˆ†")
    print(f"    â€¢ è€ƒè™‘é›†ç¾¤æƒé‡ã€åå¥½å’Œè´Ÿè½½å› å­")
    print(f"    â€¢ èƒ½å¤Ÿå®ç°çœŸæ­£çš„è´Ÿè½½å‡è¡¡")

    print(f"\nğŸ“‹ è°ƒåº¦å†³ç­–æµç¨‹å¯¹æ¯”:")
    print(f"  ç­–ç•¥å¼•æ“æµç¨‹:")
    print(f"    1. æ”¶é›†é›†ç¾¤èµ„æºå¿«ç…§")
    print(f"    2. ä½¿ç”¨ç®€åŒ–è¯„åˆ†ç­–ç•¥")
    print(f"    3. é€‰æ‹©æœ€é«˜åˆ†é›†ç¾¤")

    print(f"\n  é›†ç¾¤ç®¡ç†å™¨æµç¨‹:")
    print(f"    1. ç»´æŠ¤é›†ç¾¤å¥åº·çŠ¶æ€")
    print(f"    2. ä½¿ç”¨å¤æ‚è¯„åˆ†æœºåˆ¶")
    print(f"    3. è€ƒè™‘èµ„æºéœ€æ±‚åŒ¹é…")
    print(f"    4. åŠ¨æ€æ›´æ–°é›†ç¾¤çŠ¶æ€")


def main():
    # è¿è¡Œæœ€ç»ˆç‰ˆå¤šé›†ç¾¤å¹¶å‘è°ƒåº¦æµ‹è¯•
    cluster_dist = final_multi_cluster_concurrency_test()

    # æ¯”è¾ƒä¸åŒçš„è°ƒåº¦æ–¹æ³•
    compare_scheduling_approaches()

    print("\n" + "=" * 80)
    print("ğŸ æœ€ç»ˆæµ‹è¯•æ€»ç»“")
    print("=" * 80)

    if len(cluster_dist) > 1:
        print(f"âœ… æµ‹è¯•éªŒè¯äº†ç³»ç»Ÿå…·å¤‡è·¨é›†ç¾¤è´Ÿè½½å‡è¡¡èƒ½åŠ›")
        print(f"   â€¢ ä½¿ç”¨é›†ç¾¤ç®¡ç†å™¨çš„çœŸå®è¯„åˆ†æœºåˆ¶")
        print(f"   â€¢ ä»»åŠ¡èƒ½å¤Ÿåˆ†æ•£åˆ°å¤šä¸ªé›†ç¾¤æ‰§è¡Œ")
        print(f"   â€¢ å®ç°äº†åˆç†çš„èµ„æºåˆ©ç”¨")
    else:
        print(f"âš ï¸  æµ‹è¯•æ˜¾ç¤ºç³»ç»Ÿè°ƒåº¦å­˜åœ¨å±€é™æ€§")
        print(f"   â€¢ éœ€è¦æ”¹è¿›ç­–ç•¥å¼•æ“çš„è¯„åˆ†æœºåˆ¶")
        print(f"   â€¢ åº”æ›´å¤šä¾èµ–é›†ç¾¤ç®¡ç†å™¨çš„è¯„åˆ†èƒ½åŠ›")

    total_scheduled = sum(cluster_dist.values())
    print(f"\nğŸ“ˆ æœ€ç»ˆè°ƒåº¦ç»Ÿè®¡:")
    print(f"   â€¢ æ€»è°ƒåº¦ä»»åŠ¡: {total_scheduled}ä¸ª")
    for cluster, count in cluster_dist.items():
        print(f"   â€¢ {cluster}é›†ç¾¤: {count}ä¸ªä»»åŠ¡")


if __name__ == "__main__":
    main()