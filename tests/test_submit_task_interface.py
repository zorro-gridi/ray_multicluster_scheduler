#!/usr/bin/env python3
"""
submit_task æ¥å£å•å…ƒæµ‹è¯•
è¯Šæ–­å’Œè§£å†³ "Could not get client for cluster mac" å¼‚å¸¸é—®é¢˜
"""

import sys
import os
import time
import unittest
from unittest.mock import Mock, patch, MagicMock
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/..')

from ray_multicluster_scheduler.app.client_api.unified_scheduler import (
    UnifiedScheduler,
    initialize_scheduler_environment,
    submit_task
)
from ray_multicluster_scheduler.app.client_api.submit_task import (
    initialize_scheduler as init_task_scheduler,
    _task_lifecycle_manager
)
from ray_multicluster_scheduler.scheduler.lifecycle.task_lifecycle_manager import TaskLifecycleManager
from ray_multicluster_scheduler.scheduler.monitor.cluster_monitor import ClusterMonitor
from ray_multicluster_scheduler.scheduler.cluster.cluster_manager import ClusterManager, ClusterConfig
from ray_multicluster_scheduler.common.model import ResourceSnapshot, ClusterMetadata


class TestSubmitTaskInterface(unittest.TestCase):
    """submit_task æ¥å£å•å…ƒæµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        # æ¸…ç†å…¨å±€çŠ¶æ€
        from ray_multicluster_scheduler.app.client_api.unified_scheduler import _unified_scheduler
        if _unified_scheduler:
            _unified_scheduler._initialized = False
            _unified_scheduler.task_lifecycle_manager = None
            _unified_scheduler._config_file_path = None

        # æ¸…ç†submit_taskæ¨¡å—çš„å…¨å±€çŠ¶æ€
        from ray_multicluster_scheduler.app.client_api.submit_task import _task_lifecycle_manager, _initialization_attempted
        import ray_multicluster_scheduler.app.client_api.submit_task as submit_task_module
        submit_task_module._task_lifecycle_manager = None
        submit_task_module._initialization_attempted = False
        submit_task_module._task_results = {}

    def test_submit_task_with_mocked_components(self):
        """æµ‹è¯•submit_taskæ¥å£ä¸æ¨¡æ‹Ÿç»„ä»¶"""
        print("=" * 60)
        print("æµ‹è¯•submit_taskæ¥å£ä¸æ¨¡æ‹Ÿç»„ä»¶")
        print("=" * 60)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç®¡ç†å™¨
        cluster_manager = Mock(spec=ClusterManager)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç›‘æ§å™¨
        cluster_monitor = Mock(spec=ClusterMonitor)
        cluster_monitor.cluster_manager = cluster_manager

        # åˆ›å»ºæ¨¡æ‹Ÿçš„TaskLifecycleManager
        task_lifecycle_manager = Mock(spec=TaskLifecycleManager)
        task_lifecycle_manager.submit_task_and_get_future.return_value = "mock_result"

        # æ¨¡æ‹Ÿé›†ç¾¤é…ç½®
        cluster_configs = {
            "mac": ClusterConfig(
                name="mac",
                head_address="192.168.5.2:32546",
                dashboard="http://192.168.5.2:8265",
                prefer=True,
                weight=1.2,
                runtime_env={
                    "conda": "k8s",
                    "env_vars": {
                        "home_dir": "/Users/zorro"
                    }
                },
                tags=["macos", "arm64"]
            )
        }
        cluster_manager.clusters = cluster_configs

        # æ¨¡æ‹Ÿé›†ç¾¤ä¿¡æ¯
        cluster_info = {
            "mac": {
                "metadata": cluster_configs["mac"],
                "snapshot": ResourceSnapshot(
                    cluster_name="mac",
                    available_resources={"CPU": 8.0, "GPU": 0},
                    total_resources={"CPU": 8.0, "GPU": 0},
                    node_count=1,
                    timestamp=time.time()
                )
            }
        }
        cluster_monitor.get_all_cluster_info.return_value = cluster_info
        cluster_monitor.refresh_resource_snapshots.return_value = None

        # æ›¿æ¢å®é™…çš„TaskLifecycleManager
        with patch('ray_multicluster_scheduler.app.client_api.unified_scheduler.TaskLifecycleManager') as mock_task_lifecycle_manager_cls:
            mock_task_lifecycle_manager_cls.return_value = task_lifecycle_manager

            # åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ
            unified_scheduler = UnifiedScheduler()
            returned_manager = unified_scheduler.initialize_environment()

            # éªŒè¯TaskLifecycleManagerè¢«æ­£ç¡®åˆ›å»º
            mock_task_lifecycle_manager_cls.assert_called_once()
            self.assertEqual(returned_manager, task_lifecycle_manager)

            # åˆå§‹åŒ–submit_taskæ¨¡å—
            init_task_scheduler(task_lifecycle_manager)

            # å®šä¹‰æµ‹è¯•å‡½æ•°
            def test_function(x, y):
                return x + y

            # æäº¤ä»»åŠ¡
            task_id, result = submit_task(
                func=test_function,
                args=(1, 2),
                kwargs={},
                resource_requirements={"CPU": 1.0},
                tags=["test"],
                name="test_task",
                preferred_cluster="mac"
            )

            # éªŒè¯ç»“æœ
            self.assertIsNotNone(task_id)
            self.assertEqual(result, "mock_result")

            # éªŒè¯submit_task_and_get_futureè¢«è°ƒç”¨
            task_lifecycle_manager.submit_task_and_get_future.assert_called_once()

            print("âœ… submit_taskæ¥å£æµ‹è¯•é€šè¿‡")

    def test_submit_task_without_initialization(self):
        """æµ‹è¯•æœªåˆå§‹åŒ–æ—¶submit_taskçš„è¡Œä¸º"""
        print("\n" + "=" * 60)
        print("æµ‹è¯•æœªåˆå§‹åŒ–æ—¶submit_taskçš„è¡Œä¸º")
        print("=" * 60)

        # å®šä¹‰æµ‹è¯•å‡½æ•°
        def test_function(x, y):
            return x + y

        # å°è¯•æäº¤ä»»åŠ¡è€Œä¸åˆå§‹åŒ–è°ƒåº¦å™¨
        with self.assertRaises(Exception) as context:
            submit_task(
                func=test_function,
                args=(1, 2),
                kwargs={},
                resource_requirements={"CPU": 1.0},
                tags=["test"],
                name="test_task",
                preferred_cluster="mac"
            )

        # éªŒè¯å¼‚å¸¸ä¿¡æ¯
        self.assertIn("Scheduler not initialized", str(context.exception))
        print("âœ… æœªåˆå§‹åŒ–æ—¶æ­£ç¡®æŠ›å‡ºå¼‚å¸¸")

    def test_submit_task_with_connection_failure(self):
        """æµ‹è¯•é›†ç¾¤è¿æ¥å¤±è´¥æ—¶submit_taskçš„è¡Œä¸º"""
        print("\n" + "=" * 60)
        print("æµ‹è¯•é›†ç¾¤è¿æ¥å¤±è´¥æ—¶submit_taskçš„è¡Œä¸º")
        print("=" * 60)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç®¡ç†å™¨
        cluster_manager = Mock(spec=ClusterManager)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç›‘æ§å™¨
        cluster_monitor = Mock(spec=ClusterMonitor)
        cluster_monitor.cluster_manager = cluster_manager

        # åˆ›å»ºæ¨¡æ‹Ÿçš„TaskLifecycleManager
        task_lifecycle_manager = Mock(spec=TaskLifecycleManager)
        # æ¨¡æ‹Ÿsubmit_task_and_get_futureæŠ›å‡ºTaskSubmissionErrorå¼‚å¸¸
        from ray_multicluster_scheduler.common.exception import TaskSubmissionError
        task_lifecycle_manager.submit_task_and_get_future.side_effect = TaskSubmissionError("Could not get client for cluster mac")

        # æ¨¡æ‹Ÿé›†ç¾¤é…ç½®
        cluster_configs = {
            "mac": ClusterConfig(
                name="mac",
                head_address="192.168.5.2:32546",
                dashboard="http://192.168.5.2:8265",
                prefer=True,
                weight=1.2,
                runtime_env={
                    "conda": "k8s",
                    "env_vars": {
                        "home_dir": "/Users/zorro"
                    }
                },
                tags=["macos", "arm64"]
            )
        }
        cluster_manager.clusters = cluster_configs

        # æ¨¡æ‹Ÿé›†ç¾¤ä¿¡æ¯
        cluster_info = {
            "mac": {
                "metadata": cluster_configs["mac"],
                "snapshot": ResourceSnapshot(
                    cluster_name="mac",
                    available_resources={"CPU": 8.0, "GPU": 0},
                    total_resources={"CPU": 8.0, "GPU": 0},
                    node_count=1,
                    timestamp=time.time()
                )
            }
        }
        cluster_monitor.get_all_cluster_info.return_value = cluster_info
        cluster_monitor.refresh_resource_snapshots.return_value = None

        # æ›¿æ¢å®é™…çš„TaskLifecycleManager
        with patch('ray_multicluster_scheduler.app.client_api.unified_scheduler.TaskLifecycleManager') as mock_task_lifecycle_manager_cls:
            mock_task_lifecycle_manager_cls.return_value = task_lifecycle_manager

            # åˆå§‹åŒ–è°ƒåº¦å™¨ç¯å¢ƒ
            unified_scheduler = UnifiedScheduler()
            returned_manager = unified_scheduler.initialize_environment()

            # åˆå§‹åŒ–submit_taskæ¨¡å—
            init_task_scheduler(task_lifecycle_manager)

            # å®šä¹‰æµ‹è¯•å‡½æ•°
            def test_function(x, y):
                return x + y

            # æäº¤ä»»åŠ¡ï¼Œåº”è¯¥æŠ›å‡ºå¼‚å¸¸
            with self.assertRaises(Exception) as context:
                submit_task(
                    func=test_function,
                    args=(1, 2),
                    kwargs={},
                    resource_requirements={"CPU": 1.0},
                    tags=["test"],
                    name="test_task",
                    preferred_cluster="mac"
                )

            # éªŒè¯å¼‚å¸¸ä¿¡æ¯
            self.assertIn("Could not get client for cluster mac", str(context.exception))
            print("âœ… é›†ç¾¤è¿æ¥å¤±è´¥æ—¶æ­£ç¡®æŠ›å‡ºå¼‚å¸¸")

    def test_lazy_initialization(self):
        """æµ‹è¯•æƒ°æ€§åˆå§‹åŒ–åŠŸèƒ½"""
        print("\n" + "=" * 60)
        print("æµ‹è¯•æƒ°æ€§åˆå§‹åŒ–åŠŸèƒ½")
        print("=" * 60)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç®¡ç†å™¨
        cluster_manager = Mock(spec=ClusterManager)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç›‘æ§å™¨
        cluster_monitor = Mock(spec=ClusterMonitor)
        cluster_monitor.cluster_manager = cluster_manager

        # åˆ›å»ºæ¨¡æ‹Ÿçš„TaskLifecycleManager
        task_lifecycle_manager = Mock(spec=TaskLifecycleManager)
        task_lifecycle_manager.submit_task_and_get_future.return_value = "lazy_init_result"

        # æ¨¡æ‹Ÿé›†ç¾¤é…ç½®
        cluster_configs = {
            "mac": ClusterConfig(
                name="mac",
                head_address="192.168.5.2:32546",
                dashboard="http://192.168.5.2:8265",
                prefer=True,
                weight=1.2,
                runtime_env={
                    "conda": "k8s",
                    "env_vars": {
                        "home_dir": "/Users/zorro"
                    }
                },
                tags=["macos", "arm64"]
            )
        }
        cluster_manager.clusters = cluster_configs

        # æ¨¡æ‹Ÿé›†ç¾¤ä¿¡æ¯
        cluster_info = {
            "mac": {
                "metadata": cluster_configs["mac"],
                "snapshot": ResourceSnapshot(
                    cluster_name="mac",
                    available_resources={"CPU": 8.0, "GPU": 0},
                    total_resources={"CPU": 8.0, "GPU": 0},
                    node_count=1,
                    timestamp=time.time()
                )
            }
        }
        cluster_monitor.get_all_cluster_info.return_value = cluster_info
        cluster_monitor.refresh_resource_snapshots.return_value = None

        # å®šä¹‰æµ‹è¯•å‡½æ•°
        def test_function(x, y):
            return x + y

        # åœ¨æ²¡æœ‰æ˜¾å¼åˆå§‹åŒ–çš„æƒ…å†µä¸‹ç›´æ¥è°ƒç”¨submit_task
        with patch('ray_multicluster_scheduler.app.client_api.unified_scheduler.ClusterMonitor') as mock_cluster_monitor_cls, \
             patch('ray_multicluster_scheduler.app.client_api.unified_scheduler.TaskLifecycleManager') as mock_task_lifecycle_manager_cls:

            mock_cluster_monitor_cls.return_value = cluster_monitor
            mock_task_lifecycle_manager_cls.return_value = task_lifecycle_manager

            # æäº¤ä»»åŠ¡ï¼Œåº”è¯¥è§¦å‘æƒ°æ€§åˆå§‹åŒ–
            task_id, result = submit_task(
                func=test_function,
                args=(1, 2),
                kwargs={},
                resource_requirements={"CPU": 1.0},
                tags=["test"],
                name="lazy_init_test_task",
                preferred_cluster="mac"
            )

            # éªŒè¯ç»“æœ
            self.assertIsNotNone(task_id)
            self.assertEqual(result, "lazy_init_result")

            # éªŒè¯ClusterMonitorå’ŒTaskLifecycleManagerè¢«åˆ›å»º
            mock_cluster_monitor_cls.assert_called_once()
            mock_task_lifecycle_manager_cls.assert_called_once()

            print("âœ… æƒ°æ€§åˆå§‹åŒ–åŠŸèƒ½æµ‹è¯•é€šè¿‡")

    def test_cluster_client_connection_process(self):
        """æµ‹è¯•é›†ç¾¤å®¢æˆ·ç«¯è¿æ¥è¿‡ç¨‹"""
        print("\n" + "=" * 60)
        print("æµ‹è¯•é›†ç¾¤å®¢æˆ·ç«¯è¿æ¥è¿‡ç¨‹")
        print("=" * 60)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç®¡ç†å™¨
        cluster_manager = Mock(spec=ClusterManager)

        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç›‘æ§å™¨
        cluster_monitor = Mock(spec=ClusterMonitor)
        cluster_monitor.cluster_manager = cluster_manager

        # åˆ›å»ºçœŸå®çš„TaskLifecycleManagerå®ä¾‹ç”¨äºæµ‹è¯•
        task_lifecycle_manager = TaskLifecycleManager(cluster_monitor)

        # æ¨¡æ‹Ÿé›†ç¾¤é…ç½®
        cluster_configs = {
            "mac": ClusterConfig(
                name="mac",
                head_address="192.168.5.2:32546",
                dashboard="http://192.168.5.2:8265",
                prefer=True,
                weight=1.2,
                runtime_env={
                    "conda": "k8s",
                    "env_vars": {
                        "home_dir": "/Users/zorro"
                    }
                },
                tags=["macos", "arm64"]
            )
        }
        cluster_manager.clusters = cluster_configs

        # æ¨¡æ‹Ÿé›†ç¾¤ä¿¡æ¯
        cluster_info = {
            "mac": {
                "metadata": cluster_configs["mac"],
                "snapshot": ResourceSnapshot(
                    cluster_name="mac",
                    available_resources={"CPU": 8.0, "GPU": 0},
                    total_resources={"CPU": 8.0, "GPU": 0},
                    node_count=1,
                    timestamp=time.time()
                )
            }
        }
        cluster_monitor.get_all_cluster_info.return_value = cluster_info
        cluster_monitor.refresh_resource_snapshots.return_value = None

        # åˆå§‹åŒ–submit_taskæ¨¡å—
        init_task_scheduler(task_lifecycle_manager)

        # éªŒè¯è°ƒåº¦å™¨å·²åˆå§‹åŒ–
        from ray_multicluster_scheduler.app.client_api.submit_task import _task_lifecycle_manager as actual_task_lifecycle_manager
        self.assertIsNotNone(actual_task_lifecycle_manager)

        print("âœ… é›†ç¾¤å®¢æˆ·ç«¯è¿æ¥è¿‡ç¨‹æµ‹è¯•å‡†å¤‡å®Œæˆ")


def diagnose_connection_issue():
    """è¯Šæ–­è¿æ¥é—®é¢˜"""
    print("\n" + "=" * 60)
    print("è¯Šæ–­è¿æ¥é—®é¢˜")
    print("=" * 60)

    print("\nå¯èƒ½çš„åŸå› åˆ†æ:")
    print("1. é›†ç¾¤åœ°å€ä¸å¯è¾¾: 192.168.5.2:32546")
    print("2. é›†ç¾¤æœåŠ¡æœªå¯åŠ¨æˆ–å¼‚å¸¸")
    print("3. ç½‘ç»œè¿æ¥é—®é¢˜")
    print("4. é˜²ç«å¢™æˆ–å®‰å…¨ç»„é™åˆ¶")
    print("5. é›†ç¾¤é…ç½®é”™è¯¯")
    print("6. å®¢æˆ·ç«¯è¿æ¥æ± é—®é¢˜")

    print("\nè§£å†³æ–¹æ¡ˆå»ºè®®:")
    print("1. æ£€æŸ¥é›†ç¾¤åœ°å€æ˜¯å¦æ­£ç¡®")
    print("2. éªŒè¯é›†ç¾¤æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
    print("3. æ£€æŸ¥ç½‘ç»œè¿é€šæ€§")
    print("4. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")
    print("5. éªŒè¯é›†ç¾¤é…ç½®æ–‡ä»¶")
    print("6. é‡å¯è°ƒåº¦å™¨å’Œé›†ç¾¤æœåŠ¡")


if __name__ == "__main__":
    # è¿è¡Œå•å…ƒæµ‹è¯•
    unittest.main(exit=False)

    # è¯Šæ–­è¿æ¥é—®é¢˜
    diagnose_connection_issue()

    print("\n" + "=" * 60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print("=" * 60)