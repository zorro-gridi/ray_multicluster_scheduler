#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆçœŸå®é›†ç¾¤è¿æ¥è´Ÿè½½å‡è¡¡ç­–ç•¥éªŒè¯æµ‹è¯•
å³ä½¿åœ¨é›†ç¾¤è¿æ¥æœ‰é—®é¢˜çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®Œæˆæµ‹è¯•
"""

import sys
import os
import time
from collections import defaultdict
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/..')

from ray_multicluster_scheduler.app.client_api.unified_scheduler import (
    UnifiedScheduler,
    initialize_scheduler_environment
)
from ray_multicluster_scheduler.scheduler.policy.policy_engine import PolicyEngine
from ray_multicluster_scheduler.common.model import TaskDescription, ResourceSnapshot, ClusterMetadata


def test_load_balancing_strategy_fallback():
    """æµ‹è¯•è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼ˆå¸¦é™çº§å¤„ç†ï¼‰"""
    print("=" * 80)
    print("ğŸ” æœ€ç»ˆç‰ˆçœŸå®é›†ç¾¤è¿æ¥è´Ÿè½½å‡è¡¡ç­–ç•¥éªŒè¯æµ‹è¯•")
    print("=" * 80)

    try:
        # 1. å°è¯•åˆå§‹åŒ–è°ƒåº¦ç¯å¢ƒ
        print("ğŸ”§ å°è¯•åˆå§‹åŒ–è°ƒåº¦ç¯å¢ƒ...")
        try:
            task_lifecycle_manager = initialize_scheduler_environment()
            cluster_monitor = task_lifecycle_manager.cluster_monitor
            print("âœ… è°ƒåº¦ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")

            # 2. å°è¯•è·å–çœŸå®é›†ç¾¤ä¿¡æ¯
            print("ğŸ”„ å°è¯•è·å–çœŸå®é›†ç¾¤ä¿¡æ¯...")
            try:
                cluster_monitor.refresh_resource_snapshots(force=True)
                cluster_info = cluster_monitor.get_all_cluster_info()

                # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨é›†ç¾¤
                available_clusters = {name: info for name, info in cluster_info.items()
                                   if info and 'snapshot' in info and info['snapshot'] and
                                   info['snapshot'].available_resources}

                if available_clusters:
                    print("âœ… æˆåŠŸè·å–çœŸå®é›†ç¾¤ä¿¡æ¯")
                    return test_with_real_clusters(cluster_monitor, available_clusters)
                else:
                    print("âš ï¸  æ— æ³•è·å–å¯ç”¨é›†ç¾¤ä¿¡æ¯ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            except Exception as e:
                print(f"âš ï¸  è·å–çœŸå®é›†ç¾¤ä¿¡æ¯å¤±è´¥: {e}")
                print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
        except Exception as e:
            print(f"âš ï¸  è°ƒåº¦ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")

        # 3. ä½¿ç”¨æ¨¡æ‹Ÿé›†ç¾¤æ•°æ®è¿›è¡Œæµ‹è¯•
        print("ğŸ”§ ä½¿ç”¨æ¨¡æ‹Ÿé›†ç¾¤æ•°æ®è¿›è¡Œæµ‹è¯•...")
        return test_with_simulated_clusters()

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_with_real_clusters(cluster_monitor, available_clusters):
    """ä½¿ç”¨çœŸå®é›†ç¾¤è¿›è¡Œæµ‹è¯•"""
    print("ğŸ“‹ ä½¿ç”¨çœŸå®é›†ç¾¤ä¿¡æ¯...")

    cluster_snapshots = {}
    cluster_metadata = {}

    for cluster_name, info in available_clusters.items():
        snapshot = info['snapshot']
        metadata = info['metadata']
        cluster_snapshots[cluster_name] = snapshot
        cluster_metadata[cluster_name] = metadata

        cpu_available = snapshot.available_resources.get("CPU", 0)
        cpu_total = snapshot.total_resources.get("CPU", 0)
        gpu_available = snapshot.available_resources.get("GPU", 0)
        gpu_total = snapshot.total_resources.get("GPU", 0)

        print(f"  â€¢ {cluster_name}: CPU={cpu_available}/{cpu_total}, GPU={gpu_available}/{gpu_total}")

    # åˆ›å»ºç­–ç•¥å¼•æ“
    print("ğŸ”§ åˆ›å»ºç­–ç•¥å¼•æ“...")
    policy_engine = PolicyEngine(cluster_monitor)
    policy_engine.update_cluster_metadata(cluster_metadata)
    print("âœ… ç­–ç•¥å¼•æ“åˆ›å»ºå®Œæˆ")

    # æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦å†³ç­–
    return simulate_task_scheduling(policy_engine, cluster_snapshots)


def test_with_simulated_clusters():
    """ä½¿ç”¨æ¨¡æ‹Ÿé›†ç¾¤è¿›è¡Œæµ‹è¯•"""
    print("ğŸ“‹ ä½¿ç”¨æ¨¡æ‹Ÿé›†ç¾¤ä¿¡æ¯...")

    # æ¨¡æ‹Ÿé›†ç¾¤é…ç½®
    cluster_configs = {
        "centos": ClusterMetadata(
            name="centos",
            head_address="192.168.5.7:32546",
            dashboard="http://192.168.5.7:31591",
            prefer=False,
            weight=1.0,
            runtime_env={
                "conda": "ts",
                "env_vars": {"home_dir": "/home/zorro"}
            },
            tags=["linux", "x86_64"]
        ),
        "mac": ClusterMetadata(
            name="mac",
            head_address="192.168.5.2:32546",
            dashboard="http://192.168.5.2:8265",
            prefer=True,
            weight=1.2,
            runtime_env={
                "conda": "k8s",
                "env_vars": {"home_dir": "/Users/zorro"}
            },
            tags=["macos", "arm64"]
        )
    }

    # æ¨¡æ‹Ÿé›†ç¾¤èµ„æºå¿«ç…§
    current_time = time.time()
    cluster_snapshots = {
        "centos": ResourceSnapshot(
            cluster_name="centos",
            total_resources={"CPU": 16.0, "GPU": 0},
            available_resources={"CPU": 16.0, "GPU": 0},
            node_count=2,
            timestamp=current_time
        ),
        "mac": ResourceSnapshot(
            cluster_name="mac",
            total_resources={"CPU": 8.0, "GPU": 0, "MacCPU": 8.0},
            available_resources={"CPU": 8.0, "GPU": 0, "MacCPU": 8.0},
            node_count=1,
            timestamp=current_time
        )
    }

    # æ˜¾ç¤ºæ¨¡æ‹Ÿé›†ç¾¤ä¿¡æ¯
    for cluster_name, snapshot in cluster_snapshots.items():
        cpu_available = snapshot.available_resources.get("CPU", 0)
        cpu_total = snapshot.total_resources.get("CPU", 0)
        gpu_available = snapshot.available_resources.get("GPU", 0)
        gpu_total = snapshot.total_resources.get("GPU", 0)
        print(f"  â€¢ {cluster_name}: CPU={cpu_available}/{cpu_total}, GPU={gpu_available}/{gpu_total}")

    # åˆ›å»ºç­–ç•¥å¼•æ“
    print("ğŸ”§ åˆ›å»ºç­–ç•¥å¼•æ“...")
    policy_engine = PolicyEngine()
    policy_engine.update_cluster_metadata(cluster_configs)
    print("âœ… ç­–ç•¥å¼•æ“åˆ›å»ºå®Œæˆ")

    # æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦å†³ç­–
    return simulate_task_scheduling(policy_engine, cluster_snapshots)


def simulate_task_scheduling(policy_engine, cluster_snapshots):
    """æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦å†³ç­–"""
    print(f"\nğŸš€ æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦å†³ç­–...")
    cluster_distribution = defaultdict(int)

    # æäº¤20ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éœ€è¦2ä¸ªCPUï¼Œä¸æŒ‡å®šé›†ç¾¤
    for i in range(20):
        task_desc = TaskDescription(
            task_id=f"lb_test_task_{i}",
            name=f"è´Ÿè½½å‡è¡¡æµ‹è¯•ä»»åŠ¡{i}",
            func_or_class=lambda: None,
            args=(),
            kwargs={},
            resource_requirements={"CPU": 2.0},
            tags=["test", "load_balance"],
            preferred_cluster=None  # ä¸æŒ‡å®šé›†ç¾¤ï¼Œä½¿ç”¨è´Ÿè½½å‡è¡¡
        )

        # è®©ç­–ç•¥å¼•æ“åšè°ƒåº¦å†³ç­–
        decision = policy_engine.schedule(task_desc, cluster_snapshots)

        if decision and decision.cluster_name:
            cluster_distribution[decision.cluster_name] += 1
            print(f"    ä»»åŠ¡ {i}: è°ƒåº¦åˆ° {decision.cluster_name} - {decision.reason}")
        else:
            print(f"    ä»»åŠ¡ {i}: æ— æ³•è°ƒåº¦")

    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print(f"\nğŸ“Š è°ƒåº¦å†³ç­–ç»Ÿè®¡:")
    total_scheduled = sum(cluster_distribution.values())
    for cluster_name, count in cluster_distribution.items():
        percentage = (count / total_scheduled * 100) if total_scheduled > 0 else 0
        print(f"  â€¢ {cluster_name}: {count}ä¸ªä»»åŠ¡ ({percentage:.1f}%)")

    # åˆ†æè´Ÿè½½å‡è¡¡æ•ˆæœ
    print(f"\nğŸ“‹ è´Ÿè½½å‡è¡¡æ•ˆæœåˆ†æ:")
    if len(cluster_distribution) > 1:
        counts = list(cluster_distribution.values())
        max_count = max(counts)
        min_count = min(counts)
        balance_ratio = min_count / max_count if max_count > 0 else 0

        print(f"  âœ… å®ç°äº†è·¨é›†ç¾¤è´Ÿè½½å‡è¡¡")
        print(f"     â€¢ ä¸åŒé›†ç¾¤éƒ½æœ‰ä»»åŠ¡è¢«è°ƒåº¦")
        print(f"     â€¢ è´Ÿè½½å‡è¡¡æ¯”ç‡: {balance_ratio:.2f} (è¶Šæ¥è¿‘1è¶Šå‡è¡¡)")
    else:
        print(f"  âš ï¸  ä»»åŠ¡ä¸»è¦åœ¨å•ä¸ªé›†ç¾¤è°ƒåº¦")
        print(f"     â€¢ æœªå……åˆ†åˆ©ç”¨å¤šé›†ç¾¤èµ„æº")

    return cluster_distribution


def main():
    # è¿è¡Œè´Ÿè½½å‡è¡¡ç­–ç•¥æµ‹è¯•
    cluster_dist = test_load_balancing_strategy_fallback()

    if cluster_dist:
        print("\n" + "=" * 80)
        print("ğŸ æµ‹è¯•æ€»ç»“")
        print("=" * 80)

        if len(cluster_dist) > 1:
            print(f"âœ… è´Ÿè½½å‡è¡¡ç­–ç•¥éªŒè¯æˆåŠŸ")
            print(f"   â€¢ ä»»åŠ¡è¢«åˆ†æ•£åˆ°å¤šä¸ªé›†ç¾¤")
            print(f"   â€¢ å®ç°äº†è·¨é›†ç¾¤è´Ÿè½½å‡è¡¡")
        else:
            print(f"âš ï¸  è´Ÿè½½å‡è¡¡ç­–ç•¥æœ‰å¾…æ”¹è¿›")
            print(f"   â€¢ ä»»åŠ¡ä¸»è¦åœ¨å•ä¸ªé›†ç¾¤è°ƒåº¦")
            print(f"   â€¢ æœªå……åˆ†åˆ©ç”¨å¤šé›†ç¾¤èµ„æº")

        print(f"\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        total_scheduled = sum(cluster_dist.values())
        for cluster, count in cluster_dist.items():
            percentage = (count / total_scheduled * 100) if total_scheduled > 0 else 0
            print(f"   â€¢ {cluster}: {count}ä¸ªä»»åŠ¡ ({percentage:.1f}%)")
    else:
        print("\n" + "=" * 80)
        print("âŒ æµ‹è¯•å¤±è´¥")
        print("=" * 80)


if __name__ == "__main__":
    main()