#!/usr/bin/env python3
"""
è·¨é›†ç¾¤è°ƒåº¦æœºåˆ¶æµ‹è¯•ç”¨ä¾‹ï¼ˆä¿®å¤ç‰ˆï¼‰
éªŒè¯å½“æäº¤çš„å¹¶å‘ä»»åŠ¡æ•°å¤§äºç›®æ ‡é›†ç¾¤å¯ç”¨å¹¶å‘é‡æ—¶ï¼Œ
å‰©ä½™å¾…æ‰§è¡Œçš„ä»»åŠ¡æ˜¯å¦ä¼šè‡ªåŠ¨è¿ç§»åˆ°å…¶å®ƒç©ºé—²é›†ç¾¤è¿›è¡Œè°ƒåº¦
"""

import sys
import os
import time
import unittest
from unittest.mock import Mock, patch, MagicMock
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/..')

from ray_multicluster_scheduler.common.model import TaskDescription, ResourceSnapshot, ClusterMetadata
from ray_multicluster_scheduler.scheduler.policy.policy_engine import PolicyEngine
from ray_multicluster_scheduler.scheduler.lifecycle.task_lifecycle_manager import TaskLifecycleManager
from ray_multicluster_scheduler.scheduler.monitor.cluster_monitor import ClusterMonitor
from ray_multicluster_scheduler.scheduler.cluster.cluster_manager import ClusterManager
from ray_multicluster_scheduler.scheduler.queue.task_queue import TaskQueue


class TestCrossClusterSchedulingFixed(unittest.TestCase):
    """è·¨é›†ç¾¤è°ƒåº¦æœºåˆ¶æµ‹è¯•ï¼ˆä¿®å¤ç‰ˆï¼‰"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç®¡ç†å™¨
        self.cluster_manager = Mock(spec=ClusterManager)
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„é›†ç¾¤ç›‘æ§å™¨
        self.cluster_monitor = Mock(spec=ClusterMonitor)
        self.cluster_monitor.cluster_manager = self.cluster_manager
        
        # åˆ›å»ºä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
        self.task_lifecycle_manager = TaskLifecycleManager(self.cluster_monitor)
        
        # åˆ›å»ºä¸€ä¸ªæ›´å¤§çš„ä»»åŠ¡é˜Ÿåˆ—ä»¥é¿å…é˜Ÿåˆ—æ»¡çš„é—®é¢˜
        self.task_queue = TaskQueue(max_size=10000)
        self.task_lifecycle_manager.task_queue = self.task_queue
        
        # æ¨¡æ‹Ÿé›†ç¾¤é…ç½®
        self.cluster_configs = {
            "centos": ClusterMetadata(
                name="centos",
                head_address="192.168.5.7:32546",
                dashboard="http://192.168.5.7:31591",
                prefer=False,
                weight=1.0,
                runtime_env={
                    "conda": "ts",
                    "env_vars": {
                        "home_dir": "/home/zorro"
                    }
                },
                tags=["linux", "x86_64"]
            ),
            "mac": ClusterMetadata(
                name="mac",
                head_address="192.168.5.2:32546",
                dashboard="http://192.168.5.2:8265",
                prefer=True,
                weight=1.2,
                runtime_env={
                    "conda": "k8s",
                    "env_vars": {
                        "home_dir": "/Users/zorro"
                    }
                },
                tags=["macos", "arm64"]
            )
        }
        
        # æ¨¡æ‹Ÿé›†ç¾¤å¿«ç…§ - æ¨¡æ‹Ÿcentosé›†ç¾¤èµ„æºç´§å¼ ï¼Œmacé›†ç¾¤èµ„æºå……è¶³çš„æƒ…å†µ
        current_time = time.time()
        self.cluster_snapshots = {
            "centos": ResourceSnapshot(
                cluster_name="centos",
                total_resources={"CPU": 16.0, "GPU": 0},
                available_resources={"CPU": 2.0, "GPU": 0},  # åªæœ‰2ä¸ªCPUå¯ç”¨ï¼Œä½¿ç”¨ç‡87.5%
                node_count=3,
                timestamp=current_time
            ),
            "mac": ResourceSnapshot(
                cluster_name="mac",
                total_resources={"CPU": 8.0, "GPU": 0},
                available_resources={"CPU": 6.0, "GPU": 0},  # 6ä¸ªCPUå¯ç”¨ï¼Œä½¿ç”¨ç‡25%
                node_count=1,
                timestamp=current_time
            )
        }
        
        # æ¨¡æ‹Ÿé›†ç¾¤ä¿¡æ¯
        self.cluster_info = {
            "centos": {
                "metadata": self.cluster_configs["centos"],
                "snapshot": self.cluster_snapshots["centos"]
            },
            "mac": {
                "metadata": self.cluster_configs["mac"],
                "snapshot": self.cluster_snapshots["mac"]
            }
        }
        
        # è®¾ç½®é›†ç¾¤ç›‘æ§å™¨è¿”å›å€¼
        self.cluster_monitor.get_all_cluster_info.return_value = self.cluster_info

    def test_policy_engine_cross_cluster_decision(self):
        """æµ‹è¯•ç­–ç•¥å¼•æ“çš„è·¨é›†ç¾¤å†³ç­–é€»è¾‘"""
        print("=" * 70)
        print("æµ‹è¯•ç­–ç•¥å¼•æ“çš„è·¨é›†ç¾¤å†³ç­–é€»è¾‘")
        print("=" * 70)
        
        # åˆ›å»ºç­–ç•¥å¼•æ“
        policy_engine = PolicyEngine()
        
        # æ›´æ–°ç­–ç•¥å¼•æ“çš„é›†ç¾¤å…ƒæ•°æ®
        policy_engine.update_cluster_metadata(self.cluster_configs)
        
        # åˆ›å»ºä¸€ä¸ªä»»åŠ¡æè¿°ï¼Œä¸æŒ‡å®šé¦–é€‰é›†ç¾¤
        task_desc = TaskDescription(
            task_id="policy_test_task",
            name="policy_test_task",
            func_or_class=lambda: None,
            args=(),
            kwargs={},
            resource_requirements={"CPU": 1.0},
            tags=["test", "policy"],
            preferred_cluster=None
        )
        
        # è®©ç­–ç•¥å¼•æ“åšè°ƒåº¦å†³ç­–
        decision = policy_engine.schedule(task_desc, self.cluster_snapshots)
        
        # éªŒè¯å†³ç­–ç»“æœ
        self.assertIsNotNone(decision)
        self.assertTrue(hasattr(decision, 'cluster_name'))
        self.assertTrue(hasattr(decision, 'reason'))
        
        # ç”±äºmacé›†ç¾¤èµ„æºä½¿ç”¨ç‡æ›´ä½(25% vs 87.5%)ï¼Œç­–ç•¥å¼•æ“åº”è¯¥é€‰æ‹©macé›†ç¾¤
        self.assertEqual(decision.cluster_name, "mac")
        self.assertIn("mac", decision.reason.lower())
        
        print(f"âœ… ç­–ç•¥å¼•æ“å†³ç­–: {decision.cluster_name} - {decision.reason}")

    def test_cross_cluster_scheduling_when_preferred_cluster_overloaded(self):
        """æµ‹è¯•å½“é¦–é€‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡æ˜¯å¦ä¼šæ’é˜Ÿç­‰å¾…"""
        print("\n" + "=" * 70)
        print("æµ‹è¯•å½“é¦–é€‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡æ˜¯å¦ä¼šæ’é˜Ÿç­‰å¾…")
        print("=" * 70)
        
        # åˆ›å»ºä¸€ä¸ªä»»åŠ¡æè¿°ï¼ŒæŒ‡å®šä½¿ç”¨centosé›†ç¾¤ï¼ˆä½†è¯¥é›†ç¾¤èµ„æºç´§å¼ ï¼‰
        task_desc = TaskDescription(
            task_id="test_task_1",
            name="cross_cluster_test_task",
            func_or_class=lambda: None,
            args=(),
            kwargs={},
            resource_requirements={"CPU": 2.0},  # éœ€è¦2ä¸ªCPU
            tags=["test"],
            preferred_cluster="centos"  # æŒ‡å®šé¦–é€‰é›†ç¾¤ä¸ºcentos
        )
        
        # ç”±äºcentosé›†ç¾¤åªæœ‰2ä¸ªCPUå¯ç”¨ï¼Œåˆšå¥½æ»¡è¶³éœ€æ±‚ï¼Œä½†è¶…è¿‡é˜ˆå€¼80%
        # ç³»ç»Ÿåº”è¯¥å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…
        result = self.task_lifecycle_manager.submit_task(task_desc)
        
        # éªŒè¯ä»»åŠ¡IDè¢«è¿”å›
        self.assertEqual(result, "test_task_1")
        
        # éªŒè¯ä»»åŠ¡è¢«åŠ å…¥é˜Ÿåˆ—ï¼ˆå› ä¸ºé¦–é€‰é›†ç¾¤èµ„æºä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼ï¼‰
        self.assertIn(task_desc, self.task_lifecycle_manager.queued_tasks)
        self.assertEqual(len(self.task_lifecycle_manager.queued_tasks), 1)
        self.assertEqual(self.task_queue.size(), 1)
        
        print("âœ… é¦–é€‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡æ­£ç¡®åœ°è¢«æ”¾å…¥é˜Ÿåˆ—")

    def test_cross_cluster_scheduling_without_preferred_cluster(self):
        """æµ‹è¯•æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶çš„è·¨é›†ç¾¤è°ƒåº¦"""
        print("\n" + "=" * 70)
        print("æµ‹è¯•æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶çš„è·¨é›†ç¾¤è°ƒåº¦")
        print("=" * 70)
        
        # åˆ›å»ºä¸€ä¸ªä»»åŠ¡æè¿°ï¼Œä¸æŒ‡å®šé¦–é€‰é›†ç¾¤
        task_desc = TaskDescription(
            task_id="test_task_2",
            name="cross_cluster_test_task_no_pref",
            func_or_class=lambda: None,
            args=(),
            kwargs={},
            resource_requirements={"CPU": 2.0},  # éœ€è¦2ä¸ªCPU
            tags=["test"],
            preferred_cluster=None  # ä¸æŒ‡å®šé¦–é€‰é›†ç¾¤
        )
        
        # ç”±äºcentosé›†ç¾¤èµ„æºä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼(87.5%)ï¼Œè€Œmacé›†ç¾¤èµ„æºå……è¶³(25%)
        # ç³»ç»Ÿåº”è¯¥å°†ä»»åŠ¡è°ƒåº¦åˆ°macé›†ç¾¤
        # ä½¿ç”¨mockæ¥é¿å…å®é™…çš„Rayè°ƒç”¨
        with patch.object(self.task_lifecycle_manager.dispatcher, 'dispatch_task') as mock_dispatch:
            # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ObjectRefè€Œä¸æ˜¯å­—ç¬¦ä¸²
            mock_object_ref = Mock()
            mock_dispatch.return_value = mock_object_ref
            
            result = self.task_lifecycle_manager.submit_task(task_desc)
            
            # éªŒè¯ä»»åŠ¡IDè¢«è¿”å›
            self.assertEqual(result, "test_task_2")
            
            # éªŒè¯dispatch_taskè¢«è°ƒç”¨
            mock_dispatch.assert_called_once()
            
            print("âœ… æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶ï¼Œä»»åŠ¡è¢«æ­£ç¡®è°ƒåº¦åˆ°èµ„æºå……è¶³çš„é›†ç¾¤")

    def test_cross_cluster_scheduling_with_all_clusters_overloaded(self):
        """æµ‹è¯•æ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½æ—¶çš„ä»»åŠ¡æ’é˜Ÿæœºåˆ¶"""
        print("\n" + "=" * 70)
        print("æµ‹è¯•æ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½æ—¶çš„ä»»åŠ¡æ’é˜Ÿæœºåˆ¶")
        print("=" * 70)
        
        # æ›´æ–°é›†ç¾¤å¿«ç…§ï¼Œä½¿æ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½
        current_time = time.time()
        self.cluster_snapshots = {
            "centos": ResourceSnapshot(
                cluster_name="centos",
                total_resources={"CPU": 16.0, "GPU": 0},
                available_resources={"CPU": 1.0, "GPU": 0},  # ä½¿ç”¨ç‡93.75%ï¼Œè¶…è¿‡é˜ˆå€¼80%
                node_count=3,
                timestamp=current_time
            ),
            "mac": ResourceSnapshot(
                cluster_name="mac",
                total_resources={"CPU": 8.0, "GPU": 0},
                available_resources={"CPU": 1.0, "GPU": 0},  # ä½¿ç”¨ç‡87.5%ï¼Œè¶…è¿‡é˜ˆå€¼80%
                node_count=1,
                timestamp=current_time
            )
        }
        
        self.cluster_info["centos"]["snapshot"] = self.cluster_snapshots["centos"]
        self.cluster_info["mac"]["snapshot"] = self.cluster_snapshots["mac"]
        
        # é‡æ–°è®¾ç½®é›†ç¾¤ç›‘æ§å™¨è¿”å›å€¼
        self.cluster_monitor.get_all_cluster_info.return_value = self.cluster_info
        
        # åˆ›å»ºä¸€ä¸ªä»»åŠ¡æè¿°
        task_desc = TaskDescription(
            task_id="test_task_3",
            name="all_overloaded_test_task",
            func_or_class=lambda: None,
            args=(),
            kwargs={},
            resource_requirements={"CPU": 1.0},  # éœ€è¦1ä¸ªCPU
            tags=["test"],
            preferred_cluster=None  # ä¸æŒ‡å®šé¦–é€‰é›†ç¾¤
        )
        
        # ç”±äºæ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½ï¼Œä»»åŠ¡åº”è¯¥è¢«æ”¾å…¥é˜Ÿåˆ—
        result = self.task_lifecycle_manager.submit_task(task_desc)
        
        # éªŒè¯ä»»åŠ¡IDè¢«è¿”å›
        self.assertEqual(result, "test_task_3")
        
        # éªŒè¯ä»»åŠ¡è¢«åŠ å…¥é˜Ÿåˆ—
        self.assertIn(task_desc, self.task_lifecycle_manager.queued_tasks)
        self.assertEqual(len(self.task_lifecycle_manager.queued_tasks), 1)
        self.assertEqual(self.task_queue.size(), 1)
        
        print("âœ… æ‰€æœ‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡æ­£ç¡®åœ°è¢«æ”¾å…¥é˜Ÿåˆ—")

    def test_cross_cluster_scheduling_task_migration_simulation(self):
        """æ¨¡æ‹Ÿæµ‹è¯•ä»»åŠ¡åœ¨é›†ç¾¤èµ„æºé‡Šæ”¾åçš„è¿ç§»æœºåˆ¶"""
        print("\n" + "=" * 70)
        print("æ¨¡æ‹Ÿæµ‹è¯•ä»»åŠ¡åœ¨é›†ç¾¤èµ„æºé‡Šæ”¾åçš„è¿ç§»æœºåˆ¶")
        print("=" * 70)
        
        # é¦–å…ˆè®©æ‰€æœ‰é›†ç¾¤éƒ½è¿‡è½½ï¼Œä½¿ä»»åŠ¡è¿›å…¥é˜Ÿåˆ—
        current_time = time.time()
        self.cluster_snapshots = {
            "centos": ResourceSnapshot(
                cluster_name="centos",
                total_resources={"CPU": 16.0, "GPU": 0},
                available_resources={"CPU": 1.0, "GPU": 0},  # ä½¿ç”¨ç‡93.75%ï¼Œè¶…è¿‡é˜ˆå€¼
                node_count=3,
                timestamp=current_time
            ),
            "mac": ResourceSnapshot(
                cluster_name="mac",
                total_resources={"CPU": 8.0, "GPU": 0},
                available_resources={"CPU": 1.0, "GPU": 0},  # ä½¿ç”¨ç‡87.5%ï¼Œè¶…è¿‡é˜ˆå€¼
                node_count=1,
                timestamp=current_time
            )
        }
        
        self.cluster_info["centos"]["snapshot"] = self.cluster_snapshots["centos"]
        self.cluster_info["mac"]["snapshot"] = self.cluster_snapshots["mac"]
        self.cluster_monitor.get_all_cluster_info.return_value = self.cluster_info
        
        # æäº¤å¤šä¸ªä»»åŠ¡ä½¿å®ƒä»¬è¿›å…¥é˜Ÿåˆ—
        tasks = []
        for i in range(3):
            task_desc = TaskDescription(
                task_id=f"migration_test_task_{i}",
                name=f"migration_test_task_{i}",
                func_or_class=lambda: None,
                args=(),
                kwargs={},
                resource_requirements={"CPU": 1.0},
                tags=["test", "migration"],
                preferred_cluster=None
            )
            tasks.append(task_desc)
            result = self.task_lifecycle_manager.submit_task(task_desc)
            self.assertEqual(result, f"migration_test_task_{i}")
        
        # éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½åœ¨é˜Ÿåˆ—ä¸­
        self.assertEqual(len(self.task_lifecycle_manager.queued_tasks), 3)
        self.assertEqual(self.task_queue.size(), 3)
        print(f"âœ… {len(tasks)}ä¸ªä»»åŠ¡å·²åŠ å…¥é˜Ÿåˆ—")
        
        # ç°åœ¨æ¨¡æ‹Ÿèµ„æºé‡Šæ”¾ï¼Œä½¿macé›†ç¾¤æœ‰è¶³å¤Ÿçš„èµ„æº
        self.cluster_snapshots["mac"] = ResourceSnapshot(
            cluster_name="mac",
            total_resources={"CPU": 8.0, "GPU": 0},
            available_resources={"CPU": 6.0, "GPU": 0},  # 6ä¸ªCPUå¯ç”¨ï¼Œä½¿ç”¨ç‡25%
            node_count=1,
            timestamp=time.time()
        )
        self.cluster_info["mac"]["snapshot"] = self.cluster_snapshots["mac"]
        self.cluster_monitor.get_all_cluster_info.return_value = self.cluster_info
        
        # æ¨¡æ‹Ÿé‡æ–°è¯„ä¼°æ–¹æ³•çš„è¡Œä¸ºï¼Œä½†ä¸å®é™…è°ƒç”¨å®ƒä»¥é¿å…å¤æ‚çš„mock
        # ç›´æ¥æµ‹è¯•ç­–ç•¥å¼•æ“åœ¨è¿™ç§æƒ…å†µä¸‹ä¼šåšä»€ä¹ˆå†³ç­–
        policy_engine = PolicyEngine()
        policy_engine.update_cluster_metadata(self.cluster_configs)
        
        # æµ‹è¯•é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡æ˜¯å¦ä¼šè¢«è°ƒåº¦åˆ°macé›†ç¾¤
        task_desc = tasks[0]
        decision = policy_engine.schedule(task_desc, self.cluster_snapshots)
        
        # éªŒè¯å†³ç­–ç»“æœ
        self.assertIsNotNone(decision)
        self.assertEqual(decision.cluster_name, "mac")  # åº”è¯¥è°ƒåº¦åˆ°macé›†ç¾¤
        self.assertIn("mac", decision.reason.lower())
        
        print("âœ… èµ„æºé‡Šæ”¾åï¼Œç­–ç•¥å¼•æ“ä¼šå°†ä»»åŠ¡è°ƒåº¦åˆ°èµ„æºå……è¶³çš„macé›†ç¾¤")

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        # æ¸…ç†ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
        if hasattr(self.task_lifecycle_manager, 'running') and self.task_lifecycle_manager.running:
            self.task_lifecycle_manager.stop()


def demonstrate_cross_cluster_scheduling_behavior():
    """æ¼”ç¤ºè·¨é›†ç¾¤è°ƒåº¦è¡Œä¸º"""
    print("\n" + "=" * 70)
    print("è·¨é›†ç¾¤è°ƒåº¦è¡Œä¸ºæ¼”ç¤º")
    print("=" * 70)
    
    print("\nç³»ç»Ÿè·¨é›†ç¾¤è°ƒåº¦æœºåˆ¶è¯´æ˜:")
    print("1. é¦–é€‰é›†ç¾¤ä¼˜å…ˆ: å¦‚æœç”¨æˆ·æŒ‡å®šäº†preferred_clusterï¼Œç³»ç»Ÿä¼šä¼˜å…ˆå°è¯•è°ƒåº¦åˆ°è¯¥é›†ç¾¤")
    print("2. èµ„æºé˜ˆå€¼æ§åˆ¶: å½“é›†ç¾¤èµ„æºä½¿ç”¨ç‡è¶…è¿‡80%æ—¶ï¼Œæ–°ä»»åŠ¡ä¼šè¢«æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…")
    print("3. è´Ÿè½½å‡è¡¡: æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶ï¼Œç³»ç»Ÿä¼šé€‰æ‹©èµ„æºæœ€å……è¶³çš„é›†ç¾¤")
    print("4. åŠ¨æ€é‡è°ƒåº¦: ç³»ç»Ÿæ¯30ç§’ä¼šé‡æ–°è¯„ä¼°é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ï¼Œå°è¯•å°†å…¶è°ƒåº¦åˆ°åˆé€‚çš„é›†ç¾¤")
    print("5. ä»»åŠ¡é˜Ÿåˆ—: æ— æ³•ç«‹å³è°ƒåº¦çš„ä»»åŠ¡ä¼šè¢«ä¿å­˜åœ¨é˜Ÿåˆ—ä¸­ï¼Œç›´åˆ°æœ‰åˆé€‚èµ„æº")
    
    print("\næµ‹è¯•åœºæ™¯æ€»ç»“:")
    print("âœ“ å½“é¦–é€‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡ä¼šè¢«æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…")
    print("âœ“ æœªæŒ‡å®šé¦–é€‰é›†ç¾¤æ—¶ï¼Œä»»åŠ¡ä¼šè¢«è°ƒåº¦åˆ°èµ„æºå……è¶³çš„é›†ç¾¤")
    print("âœ“ æ‰€æœ‰é›†ç¾¤è¿‡è½½æ—¶ï¼Œä»»åŠ¡ä¼šè¢«æ”¾å…¥é˜Ÿåˆ—")
    print("âœ“ èµ„æºé‡Šæ”¾åï¼Œç­–ç•¥å¼•æ“ä¼šå°†ä»»åŠ¡è°ƒåº¦åˆ°åˆé€‚çš„é›†ç¾¤")


if __name__ == "__main__":
    # è¿è¡Œå•å…ƒæµ‹è¯•
    unittest.main(exit=False)
    
    # æ¼”ç¤ºè·¨é›†ç¾¤è°ƒåº¦è¡Œä¸º
    demonstrate_cross_cluster_scheduling_behavior()
    
    print("\n" + "=" * 70)
    print("ğŸ‰ è·¨é›†ç¾¤è°ƒåº¦æµ‹è¯•å®Œæˆ!")
    print("=" * 70)